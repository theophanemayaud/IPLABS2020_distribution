{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37f1a2543d7b5a1ffb773a616962002e",
     "grade": false,
     "grade_id": "cell-6591fb542aa1e08e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Image Processing Laboratory Notebooks</h2>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook is part of a series of computer laboratories which are designed\n",
    "to teach image-processing programming; they are running on the EPFL's Noto server. They are the practical complement of the theoretical lectures of the EPFL's Master course <b>Image Processing II</b> \n",
    "(<a href=\"https://moodle.epfl.ch/course/view.php?id=463\">MICRO-512</a>) taught by Dr. D. Sage, Dr. M. Liebling, Prof. M. Unser and Prof. D. Van de Ville.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "The project is funded by the Center for Digital Education and the School of Engineering. It is owned by the <a href=\"http://bigwww.epfl.ch/\">Biomedical Imaging Group</a>. \n",
    "The distribution or the reproduction of the notebook is strictly prohibited without the written consent of the authors.  &copy; EPFL 2021.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:0px\"><b>Authors</b>: \n",
    "    <a href=\"mailto:pol.delaguilapla@epfl.ch\">Pol del Aguila Pla</a>, \n",
    "    <a href=\"mailto:kay.lachler@epfl.ch\">Kay Lächler</a>,\n",
    "    <a href=\"mailto:alejandro.nogueronaramburu@epfl.ch\">Alejandro Noguerón Arámburu</a>,\n",
    "    <a href=\"mailto:daniel.sage@epfl.ch\">Daniel Sage</a>, and\n",
    "    <a href=\"mailto:jaejun.yoo@epfl.ch\">Jaejun Yoo</a>.\n",
    "     \n",
    "</p>\n",
    "<hr style=\"clear:both\">\n",
    "<h1>Lab 7.1: Training deep neural networks</h1>\n",
    "<div style=\"background-color:#F0F0F0;padding:4px\">\n",
    "    <p style=\"margin:4px;\"><b>Released</b>: Thursday May 27, 2021</p>\n",
    "    <p style=\"margin:4px;\"><b>Submission</b>: <span style=\"color:red\">Friday June 11, 2021</span> (before 11:59PM) on <a href=\"https://moodle.epfl.ch/course/view.php?id=463\">Moodle</a></p>\n",
    "    <p style=\"margin:4px;\"><b>Grade weigth</b>: Lab 7 (28 points), 7.5 % of the overall grade</p>\n",
    "    <p style=\"margin:4px;\"><b>Remote help</b>: Thursday 3 June and Monday 7 June, 2021 on Zoom (see Moodle for link)</p>    \n",
    "    <p style=\"margin:4px;\"><b>Related lectures</b>: Chapter 11</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: \n",
    "### SCIPER: \n",
    "\n",
    "Double-click on this cell and fill your name and SCIPER number. Then, run the cell below to verify your identity in Noto and set the seed for random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59e7a135a0be113b07efb0ac580d79ea",
     "grade": true,
     "grade_id": "cell-97b4c24c25556af9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "# This line recovers your camipro number to mark the images with your ID\n",
    "uid = int(getpass.getuser().split('-')[2]) if len(getpass.getuser().split('-')) > 2 else ord(getpass.getuser()[0])\n",
    "print(f'SCIPER: {uid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c2d728d48af23fa4206b20020a27a68",
     "grade": false,
     "grade_id": "cell-333e2f7b49cb3503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <a name=\"imports_\"></a> Imports\n",
    "In the next cell we import Python libraries we will use throughout the lab.\n",
    "* [`matplotlib.pyplot`](https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.html), to display images,\n",
    "* [`ipywidgets`](https://ipywidgets.readthedocs.io/en/latest/), to make the image display interactive,\n",
    "* [`numpy`](https://numpy.org/doc/stable/reference/index.html), for mathematical operations on arrays,\n",
    "* [PyTorch (`torch`)](https://pytorch.org/), to compare the results of neural network implementations with those of the same networks implemented in a deep learning framework with automatic differentiation (PyTorch Autograd),\n",
    "* [`sklearn`](https://scikit-learn.org/stable/), a dedicated machine learning library, that includes data preparation routines and toy datasets.\n",
    "\n",
    "We will then load the `IPLabViewer` class (see the documentation [here](https://github.com/Biomedical-Imaging-Group/IPLabImageViewer/wiki/Python-IPLabViewer()-Class) or run the Python command `help(viewer)` after loading the class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e6d6fffbbdaed83ff99f4c6050724db",
     "grade": false,
     "grade_id": "cell-615bf9f4b46043be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Configure plotting as dynamic\n",
    "%matplotlib widget\n",
    "\n",
    "# Import standard required packages for this exercise\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import torch \n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b4db5854861977cdf6f2e2a07318003",
     "grade": false,
     "grade_id": "cell-e10867c7d48215e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Training deep neural networks (20 points)\n",
    "\n",
    "In this lab, you will study the backpropagation algorithm, which is a technique to compute gradients of expressions through the recursive application of the chain rule. You will implement it for simple architectures, and explore how it is built into modern deep learning libraries like [PyTorch (`torch`)](https://pytorch.org/). Understanding this process and its subtleties will be critical for you to effectively develop, design and debug neural networks.\n",
    "\n",
    "## Index\n",
    "1. [The backpropagation algorithm](#1.-The-backpropagation-algorithm)\n",
    "    1. [Toy example: Affine function](#1.A.-Toy-example:-Affine-function-(5-points)) \n",
    "        1. [Numpy implementation](#1.A.a.-Numpy-implementation-(5-points)) **(5 points)**\n",
    "        2. [PyTorch — Introduction and implementation](#1.A.b.-PyTorch-—-Introduction-and-implementation)\n",
    "    2. [Toy example: Logistic regression](#1.B.-Toy-example:-Logistic-regression-(6-points)) **(6 points)**\n",
    "        1. [Numpy implementation](#1.B.a.-Numpy-implementation-(4-points)) **(4 points)**\n",
    "        2. [PyTorch implementation](#1.B.b.-PyTorch-implementation-(2-points)) **(2 points)**\n",
    "        3. [PyTorch implementation with higher abstractions](#1.B.c.-PyTorch-implementation-with-higher-abstractions)\n",
    "2. [Binary Classification](#2.-Binary-Classification)\n",
    "    1. [PyTorch implementation](#2.A.-PyTorch-Implementation-(9-points)) **(9 points)**\n",
    "    2. [Visualization](#2.B.-Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1423cd95f5dca9bcdf36941a3ee60b4",
     "grade": false,
     "grade_id": "cell-8fd392229b9d922a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. The backpropagation algorithm\n",
    "[Back to index](#Index)\n",
    "\n",
    "[Modern neural network architectures](https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96#:~:text=VGG16%20has%20a%20total%20of,with%20a%20stride%20of%20two.) can have millions of *learnable* parameters. Backpropagation is an algorithm that was developed for training this huge number of parameters in an efficient and effective way. The essence of this algorithm is the recursive use of the chain rule from differential calculus — you calculate the derivative of a function that was created by combining other functions for which the derivatives are known. \n",
    "\n",
    "It is fine if this does not make any sense to you at this point. We will guide you through every step of this algorithm with a series of simple toy examples.\n",
    "\n",
    "## 1.A. Toy example: Affine function (5 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Let's consider a simple network that implements a one-dimensional affine function, where $f(x,\\beta_0,\\beta_1) = \\beta_1 x + \\beta_0$ and the squared error loss function $L(\\boldsymbol{\\beta}) = (f(x,\\beta_0,\\beta_1)-y)^2$, where $\\boldsymbol{\\beta}=[\\beta_0,\\beta_1]$. Here, $x$ is an input of the neural network and $y$ denotes the desired output (label or target). This simple network can be represented as the following computational graph\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <p align=\"center\" style=\"padding: 10px\">\n",
    "            <img src=\"images/toy-example_affine-function.png\" alt=\"Computational graph of an affine function.\" style=\"width: 500px;\"><br>\n",
    "        <em style=\"color: grey\">Computational graph of the network computing the one-dimensional affine function. $a$ and $b$ are two highlighted points in the graph, for use in later explanations. In the diagram $x$ is multiplied by $\\beta_1$, which results in the value $a$ and then the constant $\\beta_0$ is added to obtain the value $b$. Finally, the loss function $L$ is computed with respect to the correct output $y$.</em>\n",
    "        </p> \n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Then, our goal is to train the network, i.e., to find the parameters $\\beta_0$ and $\\beta_1$ of the affine function, so that the loss function is minimized. From a computational point of view, training a neural network consists of the repetition of two phases\n",
    "\n",
    "1. the forward pass, to compute the value of the loss function and the intermediate values,\n",
    "2. the backward pass, to compute the gradients of the loss function with respect to every learnable parameter. These are then used to update the learnable parameters to _better_ values, and the procedure is repeated.\n",
    "\n",
    "The forward pass is pretty straightforward. The output of one layer is the input of the next, so one computes each operation in the graph from left to right. The backward pass is a bit more complicated: it requires us to use the chain rule to compute **the gradient of the loss function with respect to each of the learnable parameters** (also known as weights). Here, the backward process of this toy-example network can be described by the following formulas\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\beta_0} = \\frac{\\partial L}{\\partial b}\\frac{\\partial b}{\\partial \\beta_0}\\mbox{, and }\\frac{\\partial L}{\\partial \\beta_1} = \\frac{\\partial L}{\\partial b}\\frac{\\partial b}{\\partial a}\\frac{\\partial a}{\\partial \\beta_1}\\,.$$\n",
    "\n",
    "We will begin by implementing this chain rule to compute the gradient $\\nabla L = \\left[\\frac{\\partial L}{\\partial \\beta_0}, \\frac{\\partial L}{\\partial \\beta_1} \\right]^{\\mathrm{T}}$. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> If you don't remember how the chain rule works, you can revise <a href=\"https://moodle.epfl.ch/pluginfile.php/2914036/mod_resource/content/1/exercise_8.pdf\">the last exercise collection</a>. In any case, read through the code provided below and make sure you understand what each line is doing.\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "<b>Note:</b> In the notation above, we are purposely hiding something away. Where are each of these derivatives evaluated? Make sure you fully understand this before moving on. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9963dad109af5e148b47d72b2d256420",
     "grade": false,
     "grade_id": "cell-c964c751338c3cb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.a. Numpy implementation (5 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Here, you will implement the forward and backward evaluations for the toy example network above using NumPy. You will do this manually, as opposed to using an _autodifferentiation_ package like PyTorch, as we will explore [further below](#1.A.b.-PyTorch-—-Introduction-and-implementation). \n",
    "\n",
    "### Forward pass\n",
    "\n",
    "**For 1 point**, complete the function `model_forward` that defines the forward pass in the cell below.\n",
    "\n",
    "The function `model_forward` takes as inputs\n",
    " * `x`: the input value, \n",
    " * `betas` (tuple of two scalars): the two parameters ($\\beta_0$, $\\beta_1$),\n",
    " \n",
    "and returns the values $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f84b771fad4630d00bbe50b3fef442",
     "grade": false,
     "grade_id": "cell-7d7658ca445171e7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the forward pass\n",
    "def model_forward(x, betas):\n",
    "    # Extract parameters from tuple and initialize a and b\n",
    "    beta0, beta1 = betas\n",
    "    a = None\n",
    "    b = None\n",
    "    \n",
    "    # Compute affine function (compute both a and b)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edadcd8acd48b498e2f13fa877e4cb0f",
     "grade": false,
     "grade_id": "cell-b552f0f4077e8e20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, run the next cell to actually perform a forward pass and test your function. We will use the following input values and parameters: $x = -2$, $\\beta_0 = 2$, $\\beta_1 = -4$, $y = 20$. Verify that your implementation is correct by confirming your results match what you expect.\n",
    "\n",
    "Run the next cell to test your function with these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bafdfa28505c6b76d505217dbd0b01a",
     "grade": true,
     "grade_id": "cell-479eb5e721b5b8a9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the values of input and parameters\n",
    "x = np.array(-2.)\n",
    "beta0 = np.array(2.)\n",
    "beta1 = np.array(-4.) \n",
    "# Set the value of the target / label\n",
    "y = np.array(20.)\n",
    "\n",
    "# Run the forward pass\n",
    "a, b = model_forward(x, (beta0, beta1))\n",
    "\n",
    "# Compute the loss for the current output\n",
    "L = (b - y)**2\n",
    "\n",
    "# Print results\n",
    "print(f\"Parameters: β₀ = {beta0} and β₁ = {beta1}.\\n\\\n",
    "Forward pass, results: a = {a}, b = {b}, with y = {y}, \\\n",
    "resulting in a loss of L = {L}.\\n\")\n",
    "\n",
    "# Sanity check\n",
    "assert a == 8, 'a is not correct!'\n",
    "assert b == 10, 'b is not correct!'\n",
    "assert L == 100, 'L is not correct!'\n",
    "print('Congratulations! Your forward pass seems to be working.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75aaaefb73762a104d6ffbf907d4fdc6",
     "grade": false,
     "grade_id": "cell-32b232070bc46072",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By looking at $L$, it is clear that the parameters $\\beta_1$ and $\\beta_0$ could improve. Let us look further into it. \n",
    "\n",
    "### Backward pass\n",
    "\n",
    "To begin this section, and **for 2 points**, write **the value** of the partial derivatives used in the expression above (**each worth 0.5 points**) when $x=-2$, $y=20$, $\\beta_0=2$, and $\\beta_1=-4$.\n",
    "\n",
    "To answer, change the value of the variables `dLdb` (for $\\partial L / \\partial b$), `dbda` (for $\\partial b/\\partial a$), `dbdbeta0` (for $\\partial b/\\partial \\beta_0$) and `dadbeta1` (for $\\partial a/\\partial \\beta_1$). To use them later effectively, **provide *NumPy* arrays of shape `(1, )` of floating point type**, as in the example variable `example` below.\n",
    "\n",
    "Run the four cells after this one to check that your answers are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed7182a6d8e3c6066b853613581926dc",
     "grade": false,
     "grade_id": "cell-49281cbb156627ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# dL/db\n",
    "dLdb = None\n",
    "# db/da\n",
    "dbda = None \n",
    "# da/dbeta1\n",
    "dadbeta1 = None\n",
    "# db/dbeta0\n",
    "dbdbeta0 = None\n",
    " \n",
    "\n",
    "# For later use, please set the values as floats, with shape (1, ) e.g.,\n",
    "example = np.array([1.])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa4f3c4a4f1c22b9856328d167eda49e",
     "grade": false,
     "grade_id": "cell-c40c67781c25f87a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> Now, take the time to consider: following the chain rule we showed you at <a href=\"#1.A.-Toy-example:-Affine-function-(5-points)\">the start of the section</a>, what are the values of $\\partial L / \\partial \\beta_0$ and $\\partial L / \\partial \\beta_1$ in these same conditions? Does this coincide with your manual computations?  \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "650069a5aeddd8f49f548818856ede1e",
     "grade": true,
     "grade_id": "cell-773f9092141c3cc8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(dLdb) == np.ndarray, 'Make sure that you provide a NumPy array like in the example variable above.'\n",
    "assert dLdb.shape == (1, ), f'This is a one-dimensional partial derivative. The shape of the array should be (1, ) instead of {dLdb.shape}.' \n",
    "assert dLdb.dtype == float, 'Make sure that you provide a floating-point number by ending whole numbers in a dot like in the example variable above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae92821432fab7ab7494bb6bd526e9ba",
     "grade": true,
     "grade_id": "cell-1fe0e4b6e265edae",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(dbda) == np.ndarray, 'Make sure that you provide a NumPy array like in the example variable above.'\n",
    "assert dbda.shape == (1, ), f'This is a one-dimensional partial derivative. The shape of the array should be (1, ) instead of {dbda.shape}.' \n",
    "assert dbda.dtype == float, 'Make sure that you provide a floating-point number by ending whole numbers in a dot like in the example variable above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b26aa88ab89d1351ae0dbce80dc0660",
     "grade": true,
     "grade_id": "cell-37f40a5db426c230",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(dadbeta1) == np.ndarray, 'Make sure that you provide a NumPy array like in the example variable above.'\n",
    "assert dadbeta1.shape == (1, ), f'This is a one-dimensional partial derivative. The shape of the array should be (1, ) instead of {dadbeta1.shape}.' \n",
    "assert dadbeta1.dtype == float, 'Make sure that you provide a floating-point number by ending whole numbers in a dot like in the example variable above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68df2fd72cc42148842c84a705baf0f8",
     "grade": true,
     "grade_id": "cell-9fc03908862b9f53",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(dbdbeta0) == np.ndarray, 'Make sure that you provide a NumPy array like in the example variable above.'\n",
    "assert dbdbeta0.shape == (1, ), f'This is a one-dimensional partial derivative. The size of the array should be (1, ) instead of {dbdbeta0.size}.' \n",
    "assert dbdbeta0.dtype == float, 'Make sure that you provide a floating-point number by ending whole numbers in a dot like in the example variable above.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91675e8e49e0c4dda17825b9d824c2ce",
     "grade": false,
     "grade_id": "cell-c0a58f81ee1b0137",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, **for 2 points**, complete the function `model_forward_backward` below, which implements the forward and backward passes of the toy example network above, and **works for arbitrary values of $x$, $y$, $\\beta_0$ and $\\beta_1$**. Your function will take as parameters\n",
    "* `x`: the input value,\n",
    "* `y`: the target or label value,\n",
    "* `betas`: the parameters $\\beta_0$ and $\\beta_1$, given in a tuple like `betas=(beta0,beta1)`,\n",
    "\n",
    "and will output the value of the loss function and its derivatives with respect to $\\beta_0$ and $\\beta_1$ (its gradient with respect to the weights). Follow the comments that will guide you on how to calculate $\\partial L / \\partial \\beta_0$ and $\\partial L / \\partial \\beta_1$ (if in doubt, you can reread [Section 1.A](#1.A.-Toy-example:-Affine-function-(5-points))).\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "\n",
    "<b>Hint:</b> You can reuse your function `model_forward(x, betas)` here.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f20c3412f1101b8611b512c01d25daf",
     "grade": false,
     "grade_id": "cell-2d5e5c9d27ccc509",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the forward pass and the backward pass (backpropagation)\n",
    "def model_forward_backward(x, y, betas):    \n",
    "    # Extract parameters from tuple\n",
    "    beta0, beta1 = betas\n",
    "    \n",
    "    # Initialize variables (make sure to change all their values)\n",
    "    a = None\n",
    "    b = None\n",
    "    L = None\n",
    "    dLdb = None \n",
    "    dbdbeta0 = None\n",
    "    dLdbeta0 = None\n",
    "    dbda = None\n",
    "    dadbeta1 = None\n",
    "    dLdbeta1 = None\n",
    "    \n",
    "    # Perform the forward pass\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute the value of the loss function\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Perform the backward pass - backpropagate through L and b - to find dLdbeta0 and dLdbeta1.\n",
    "    # To find dLdbeta0 you will need dLdb and dbdbeta0\n",
    "    # Compute dLdb using b and y\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute dbdbeta0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute dLdbeta0 using dLdb and dbdbeta0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Great! Now you are just missing dLdbeta1. You will need dbda and dadbeta1 (you already have dLdb) \n",
    "    # Compute dbda\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute dadbeta1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute dLdbeta1 using dLdb, dbda, dadbeta1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return L, (dLdbeta0, dLdbeta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcab7d00cba3702e4a84b52158a124f7",
     "grade": false,
     "grade_id": "cell-1b6025470a086828",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the following cell to test your function. We will first test with the original parameters we were using before, i.e., $x=-2$, $y=20$, $\\beta_0=2$, and $\\beta_1=-4$.\n",
    "\n",
    "However, there are sliders so that you can change the values to whatever you want. Click the button `Forward Backward` to rerun `model_forward_backward` with the new parameters and update the textbox. \n",
    "\n",
    "<div class = 'alert alert-warning'>\n",
    "\n",
    "<b>Note</b>: The sanity check on your function will appear when you first run the cell. Look at the textbox before changing the parameters! If your function has any problem, try to work out the math manually and find the error in your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6576ab10c5b5d280291eed55979d517a",
     "grade": true,
     "grade_id": "cell-c95fb9fff1aa7c5d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Declare widgets\n",
    "x_slider = widgets.FloatSlider(value = -2, min = -20, max = 20, step = 0.1, description = r'$x$')\n",
    "y_slider = widgets.FloatSlider(value = 20, min = -20, max = 20, step = 0.1, description = r'$y$')\n",
    "beta0_slider = widgets.FloatSlider(value = 2, min = -20, max = 20, step = 0.1, description = r'$\\beta_0$')\n",
    "beta1_slider = widgets.FloatSlider(value = -4, min = -20, max = 20, step = 0.1, description = r'$\\beta_1$')\n",
    "text_box = widgets.Textarea(value = '', layout={'width': '500px', 'height': '70px'})\n",
    "button = widgets.Button(description = 'Forward Backward')\n",
    "\n",
    "def interact(change):\n",
    "    '''\n",
    "    Function that gets values of widgets and calls forward and backward pass\n",
    "    '''\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    beta0 = beta0_slider.value\n",
    "    beta1 = beta1_slider.value\n",
    "    \n",
    "    L, (dLdbeta0, dLdbeta1) = model_forward_backward(x, y, (beta0, beta1))\n",
    "    # Test your function when you first run the cell\n",
    "    if x == -2 and y == 20 and beta0 == 2 and beta1 == -4:\n",
    "        string = ''\n",
    "        if not np.isclose(L, 100): string += f'L is not correct. Should be 100 instead of {L}\\n'\n",
    "        if not np.isclose(dLdbeta0, -20): string += f'dL/dβ₀ is not correct. Should be -20 instead of {dLdbeta0}\\n'\n",
    "        if not np.isclose(dLdbeta1, 40): string += f'dL/dβ₁ is not correct. Should be 40 instead of {dLdbeta1}\\n'\n",
    "        if not string == '': string = 'WARNING:\\n' + string\n",
    "        else: string = 'Congratulations! Your function seems correct.\\n'\n",
    "        text_box.value = string + f'Values: x = {x}, y = {y}, β₀ = {beta0}, β₁ = {beta1}\\n\\\n",
    "Results: L = {L}, dL/dβ₀ = {dLdbeta0}, dL/dβ₁ = {dLdbeta1}'\n",
    "        \n",
    "    # Display any other values\n",
    "    else:\n",
    "        text_box.value = f'Values: x = {x:.2f}, y = {y:.2f}, β₀ = {beta0:.2f}, β₁ = {beta1:.2f}\\n\\\n",
    "Results: L = {L:.2f}, dL/dβ₀ = {dLdbeta0:.2f}, dL/dβ₁ = {dLdbeta1:.2f}'\n",
    "\n",
    "# Link button to function and display all widgets\n",
    "button.on_click(interact)\n",
    "display(widgets.HBox([widgets.VBox([x_slider, y_slider, beta0_slider, beta1_slider]), \n",
    "                      widgets.VBox([text_box, button])]))\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e201789a73710df88e76766df52b4d2a",
     "grade": false,
     "grade_id": "cell-b2ee6149a3561f9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> Regardless of the sanity check, does the math work out? What does each of the partial derivatives mean here? Did you try to work out an example where the loss is zero?\n",
    "    \n",
    "So far we have defined and implemented the architecture of our toy example, and calculated one forward and backward pass to work out the gradients of the loss $L$ with respect to the weights $\\beta_0$ and $\\beta_1$. In the next section, we will see how to change the values of $\\beta_0$ and $\\beta_1$ to reduce the loss.\n",
    "</div> \n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Our goal is to find the values of $\\beta_0$ and $\\beta_1$ that minimize the loss function. We can exploit $\\partial L/\\partial \\beta_0$ and $\\partial L/\\partial \\beta_1$ for a [gradient descent algorithm](https://en.wikipedia.org/wiki/Gradient_descent), one of the simplest and most fundamental optimization algorithms in machine learning (ML), science, and engineering. The general idea is to update a parameter (for example, a vector $\\boldsymbol{\\alpha}$) in the direction that minimizes the gradient of a function $f$. The parameter $\\boldsymbol{\\alpha}_t$ at iteration $t+1$, then, is given by the following update rule\n",
    "\n",
    "$$\\boldsymbol{\\alpha}_{t+1} \\leftarrow \\boldsymbol{\\alpha}_t - \\gamma \\nabla_{\\boldsymbol{\\alpha}} f(\\boldsymbol{\\alpha}_t)\\,.$$\n",
    "\n",
    "In our context, $f=L$ is the loss function and $\\gamma$ is the *learning rate*, an [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) to be chosen by the user. The gradient $\\nabla_{\\boldsymbol{\\alpha}}$ is taken with respect to the parameter, which explains why we need to perform a backward pass and extract $\\frac{\\partial L}{\\partial \\beta_0}$ and $\\frac{\\partial L}{\\partial \\beta_1}$ in our toy example.\n",
    "\n",
    "Run the next cell to declare the function `gradient_descent`, and make sure to understand it completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f40cbd0dac0d22e3ebc79939f8d046fe",
     "grade": false,
     "grade_id": "cell-337db23113c93026",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate, betas, gradient):\n",
    "    # Extract parameters and partial derivatives\n",
    "    beta0, beta1 = betas\n",
    "    dLdbeta0, dLdbeta1 = gradient\n",
    "    # Take gradient step\n",
    "    beta0_updated = beta0 - learning_rate * dLdbeta0\n",
    "    beta1_updated = beta1 - learning_rate * dLdbeta1\n",
    "    # Return updated parameters\n",
    "    return beta0_updated, beta1_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fafc4c65eeece895206e977afdb38526",
     "grade": false,
     "grade_id": "cell-32dbe1ce2fa66c6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the following cell, where we will redeclare our parameters with the original values defined at the beginning of [Section 1.A.a](#1.A.a.-Numpy-implementation-(5-points)) and perform one step of the gradient descent algorithm. Observe what happens when running the cell, and go trough every line of code. Note that we have arbitrarily chosen `learning_rate = 1e-2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e576264d3572a2fa8fdb139a1853b25c",
     "grade": false,
     "grade_id": "cell-da3b0991d13f87ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the value of input\n",
    "x = np.array(-2.)\n",
    "# Set the value of the target / label\n",
    "y = np.array(20.)\n",
    "# Set the initial parameters \n",
    "beta0 = np.array(2.)\n",
    "beta1 = np.array(-4.) \n",
    "betas = (beta0, beta1)\n",
    "# Set the learning rate for the gradient descent step\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Make a forward - backward pass\n",
    "L, gradient = model_forward_backward(x, y, betas)\n",
    "# Take the gradient descent step\n",
    "betas_new = gradient_descent(learning_rate, betas, gradient)\n",
    "\n",
    "# Make a forward pass to evaluate the new loss function\n",
    "a, b = model_forward(x, betas_new)\n",
    "# Compute the loss after the update\n",
    "L_new = (b-y)**2\n",
    "\n",
    "print(f'Initial condition: β₀ = {beta0}, β₁ = {beta1},  L = {L}.')\n",
    "print(f'Updated condition: β₀ = {betas_new[0]}, β₁ = {betas_new[1]},  L = {L_new}.')\n",
    "if L_new < L:\n",
    "    print(f'Congratulations! Your loss function has been reduced from {L} to {L_new}.')\n",
    "else:\n",
    "    raise Exception(f'Check your implementation: your loss went up from {L} to {L_new} after an iteration of gradient descent.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d9471746bb7ddf184d45c250b1dcc29",
     "grade": false,
     "grade_id": "cell-f5bbeb0cd72a9966",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we will actually let gradient descent run until convergence (or for a larger number of steps). In the next cell, we will plot the evolution of the loss and of the parameters $\\beta_0$ and $\\beta_1$ with every iteration of the algorithm. Now: you might be asking yourself what to do about the learning rate. Indeed, it is a parameter that, when the shape of the loss function is unknown, has to be tuned manually and can take a wide range of values. After running the next cell, you will find several sliders and two buttons:\n",
    "\n",
    "* `gamma_value`: to choose the value of the learning rate,\n",
    "* `gamma_decimal`: to choose the order of magnitude of the learning rate,\n",
    "* `iterations_slider`: to choose the number of iterations for which gradient descent will be run,\n",
    "* `button` and `button_res`: to run gradient descent once more and to clear the plot, respectively.\n",
    "\n",
    "Go carefully through the function `gradient_descent_full_run` to see how we run the algorithm. Then run the cell to interactively see the performance (note that we are working with the same parameters we declared at the beginning of [Section 1.A.a](#1.A.a.-Numpy-implementation-(5-points))). \n",
    "\n",
    "<div class = 'alert alert-warning'>\n",
    "\n",
    "<b>Hint</b>: Play with different values of the learning rate, and you can see how important it is to choose an appropiate one. To clear the axes, click the button `Clear Axes`. You can see the learning rate that you are about to choose in the box between the sliders.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48a46f0a101d010e734ad8024eecdc9d",
     "grade": false,
     "grade_id": "cell-52052abfcce643ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Declare sliders\n",
    "gamma_value = widgets.IntSlider(value = 1, min = 1, max = 10, step = 1, description = r'$\\gamma$ val.')\n",
    "gamma_decimal = widgets.IntSlider(value = -2, min = -5, max = 5, step = 1, description = r'1E')\n",
    "lr_textbox = widgets.Textarea(value = f'{10**-2:.0E}', description = r'$\\gamma =$', layout = {'width': '200px', 'height': '30px'})\n",
    "iterations_slider = widgets.IntSlider(value = 100, min = 10, max = 500, step = 10, description = 'Iterations')\n",
    "table = widgets.Textarea(value = '', layout = {'width': '55%', 'height': '300px'})\n",
    "button = widgets.Button(description = 'Run GD')\n",
    "button_res = widgets.Button(description='Clear Axes')\n",
    "# Prepare display\n",
    "box = widgets.HBox([table, widgets.VBox([gamma_value, gamma_decimal, lr_textbox, iterations_slider, button, button_res])])\n",
    "\n",
    "# Declare figure to plot evolution of parameters\n",
    "plt.close('all')\n",
    "fig = plt.figure(num = \"Convergence of gradient descent\", figsize = [9,5])\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax_loss = fig.add_subplot(121); ax_loss.set_title('Loss vs Iterations'); ax_loss.set_ylabel(\"Loss [dB]\")\n",
    "ax_b0 = fig.add_subplot(222); ax_b0.set_title(r'Parameter $\\beta_0$ vs Iterations'); ax_b0.set_ylabel(r'$\\beta_0$')\n",
    "ax_b1 = fig.add_subplot(224); ax_b1.set_title(r'Parameter $\\beta_1$ vs Iterations'); ax_b1.set_ylabel(r'$\\beta_1$')\n",
    "plt.tight_layout(); plt.show()\n",
    "display(box)\n",
    "\n",
    "# # Declare function that performs a full pass of the gradient descent algorithm\n",
    "def gradient_descent_full_run(change):\n",
    "    \n",
    "    # Set learning rate and number of itersations\n",
    "    learning_rate = gamma_value.value * 10 ** gamma_decimal.value\n",
    "    nrof_iterations = iterations_slider.value\n",
    "    \n",
    "    # Set printing frequency (every how many iterations we want to print in the table)\n",
    "    printing_freq = 10\n",
    "\n",
    "    ## Run gradient descent and print results' table\n",
    "    betas_new = (beta0, beta1)\n",
    "    # Prepare results' table\n",
    "    table_str = '|\\tIteration\\t|\\tβ₀\\t\\t\\t|\\tβ₁\\t\\t\\t|\\tPrevious loss (L_prev)\\t|\\n'+97*'-'+'\\n'\n",
    "    table_str += f'|\\t{0:3}\\t\\t|\\t{beta0:.4E}\\t|\\t{beta1:.4E}\\t|\\t-\\t\\t\\t\\t\\t|\\n'\n",
    "    # Initialize variables for plots\n",
    "    its, beta0s, beta1s, losses = np.arange(0,nrof_iterations+1), np.empty(nrof_iterations+1), np.empty(nrof_iterations+1), np.empty(nrof_iterations+1)\n",
    "    # Fill initial value\n",
    "    beta0s[0], beta1s[0] = beta0, beta1\n",
    "    # Iterate\n",
    "    for iter_num in range(nrof_iterations):\n",
    "        # Pack the parameters together\n",
    "        betas = betas_new\n",
    "        # Make a forward - backward pass\n",
    "        L_prev, gradient = model_forward_backward(x, y, betas)\n",
    "        # Take the gradient descent step\n",
    "        betas_new = gradient_descent(learning_rate, betas, gradient)\n",
    "        # Store variables and inform of progress\n",
    "        beta0s[iter_num+1], beta1s[iter_num+1], losses[iter_num] = betas_new[0], betas_new[1], L_prev \n",
    "        if (iter_num+1)%printing_freq == 0:\n",
    "            table_str += f'|\\t{iter_num+1:3}\\t\\t|\\t{betas_new[0]:.4E}\\t|\\t{betas_new[1]:.4E}\\t|\\t{L_prev:.3E}\\t\\t\\t|\\n'\n",
    "    # Fill last value, close table and display\n",
    "    losses[-1], _ = model_forward_backward(x, y, betas_new)\n",
    "    table_str += 97*'-'\n",
    "    table.value = table_str\n",
    "    \n",
    "    ## Update plot\n",
    "    ax_loss.plot(np.arange(0,nrof_iterations+1), 10*np.log10(losses), label = f'$\\gamma$={learning_rate:.2E}'); ax_loss.legend()\n",
    "    ax_b0.plot(np.arange(0,nrof_iterations+1),beta0s, label = f'$\\gamma$={learning_rate:.0E}'); ax_b0.legend()\n",
    "    ax_b1.plot(np.arange(0,nrof_iterations+1),beta1s, label = f'$\\gamma$={learning_rate:.0E}'); ax_b1.legend()\n",
    "    \n",
    "def clear_axes(change):\n",
    "    ax_loss.clear(); ax_loss.set_title('Loss vs Iterations'); ax_loss.set_ylabel(\"Loss\")\n",
    "    ax_b0.clear(); ax_b0.set_title(r'Parameter $\\beta_0$ vs Iterations'); ax_b0.set_ylabel(r'$\\beta_0$')\n",
    "    ax_b1.clear(); ax_b1.set_title(r'Parameter $\\beta_1$ vs Iterations'); ax_b1.set_ylabel(r'$\\beta_1$')\n",
    "\n",
    "def change_lr(change):\n",
    "    learning_rate = gamma_value.value * 10 ** gamma_decimal.value\n",
    "    lr_textbox.value = f'{learning_rate:.0E}'\n",
    "\n",
    "gamma_value.observe(change_lr, 'value')\n",
    "gamma_decimal.observe(change_lr, 'value')\n",
    "# Link buttons to functions and run once\n",
    "button.on_click(gradient_descent_full_run)\n",
    "button_res.on_click(clear_axes)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ada03685f0f283ec8084e30c507df02",
     "grade": false,
     "grade_id": "cell-30e9e12694454f91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> Towards which values do $\\beta_0$ and $\\beta_1$ converge? Does this make sense? What happens if <code>learning_rate</code> is too big (e.g., <code>0.5</code> or <code>5</code>) or too small (e.g., <code>1e-10</code>)? What happens if you set it <i>just right</i>, to <code>1e-1</code>? Make sure to explore those questions!\n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08503c05b8084946be91024161451a66",
     "grade": false,
     "grade_id": "cell-01c94462338a424f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.b. PyTorch — Introduction and implementation\n",
    "[Back to index](#Index)\n",
    "\n",
    "\n",
    "As this example may have started to show you, implementing the backward pass manually is not always feasible (don't worry, you will get a much clearer idea why in [the next section](#1.B.-Toy-example:-Logistic-regression-(6-points))). In this section, we will introduce [PyTorch](https://pytorch.org/), a widely used deep learning framework based on Python. We will also show you how to implement and train the toy example network above using PyTorch.\n",
    "\n",
    "#### Tensors\n",
    "\n",
    "The `Tensor` is the data structure that serves as the fundamental building block of PyTorch. A `Tensor` is just like a NumPy array (`ndarray`). However, unlike the NumPy array, it is designed specifically for deep learning. In particular, 1) any operation on tensors can take advantage of the parallel computation capabilities of graphical processing units (GPU), and 2) the backward pass can be automatically obtained by asking PyTorch to construct the computational graph and use automatic built-in backpropagation algorithms (also known as [_Autograd_](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)  or automatic differentiation algorithms). \n",
    "\n",
    "To create a tensor from a NumPy array, one uses the `torch.tensor` function, which asks for the parameters\n",
    "* `data`: a NumPy array with the desired initial value of the tensor,\n",
    "* `requires_grad`: a boolean value, set to `True` for trainable parameters, but `False` by default. This includes them into PyTorch's dynamic computational graph, allowing for backpropagation to obtain the gradient of the loss function, and\n",
    "* `dtype`: the data type to be used for the tensor. Although this is usually derived automatically from `data`, it is good to set it specifically if we want to ensure it and the way we initialize it leaves room for doubt. For example, here we are initializing variables that we want to represent as double-precision floating-point numbers (`torch.float64`) with integer values.\n",
    "\n",
    "Run the next cell to create the tensors necessary to implement the toy example network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf2ab39fa441c47162663a020637b57e",
     "grade": false,
     "grade_id": "cell-e04c8da19ed0519e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the tensors, initializing them to the variables' values\n",
    "x_tc     = torch.tensor(-2, requires_grad = False, dtype=torch.float64)\n",
    "beta0_tc = torch.tensor(2,  requires_grad = True,  dtype=torch.float64)\n",
    "beta1_tc = torch.tensor(-4, requires_grad = True,  dtype=torch.float64)\n",
    "y_tc     = torch.tensor(20, requires_grad = False, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99fcaf8b46fec21325e76ef1eace587b",
     "grade": false,
     "grade_id": "cell-55845f40cc4b8573",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Forward pass in PyTorch\n",
    "\n",
    "Despite `Tensor` being a new data structure with these special features, this does not change much when implementing the expressions in the forward pass. In fact, thanks to the magic of object-oriented programming and [dynamic dispatch](https://en.wikipedia.org/wiki/Dynamic_dispatch) (_warning_: technical, only for those with genuine interest in programming), we can simply run the function `model_forward` we defined above, but with the newly defined tensors as arguments.\n",
    "\n",
    "Run the following cell to define the computational graph of the toy example network above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae6b335dd52e7f972a3ac7c75a46ab69",
     "grade": false,
     "grade_id": "cell-9be2f41494702d33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the forward pass\n",
    "a_tc, b_tc = model_forward(x_tc, (beta0_tc, beta1_tc))\n",
    "\n",
    "# Compute the loss function\n",
    "L_tc = (b_tc - y_tc)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d555b7e303bef5c5a48f978ff3624599",
     "grade": false,
     "grade_id": "cell-5d97f01e98253b19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that `requires_grad` is *contagious*. In other words, any `Tensor` created by operating on at least one `Tensor` object with `requires_grad = True` will also get `requires_grad = True`. If you review the computations you needed to do in the backward pass, you will quickly understand why: PyTorch needs to make sure that it can go backwards through the graph, from the loss function to any variable with `requires_grad = True`, and so it needs to keep track of all the by-products (_branches_) that result from each of these variables (_leafs_).\n",
    "\n",
    "Run the following cell to verify this empirically with `L_tc` and `a_tc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b95e645f0c2e6e73cb3eb7ec3e8a6a",
     "grade": false,
     "grade_id": "cell-645b230d7442bf46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'Although requires_grad is set to {x_tc.requires_grad} for x_tc and to {y_tc.requires_grad} for y_tc,')\n",
    "print(f'it was automatically set to {L_tc.requires_grad} for L_tc and to {a_tc.requires_grad} for a_tc.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36495a0bca2bf1fb0d0103e5be5352bb",
     "grade": false,
     "grade_id": "cell-426459dbb7451fe0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Backward pass in PyTorch\n",
    "\n",
    "Thanks to the built-in backpropagation algorithms, we do not need to do much to obtain the gradient of the loss function with respect to the trainable parameters. We simply call the method `L_tc.backward()`, which performs the backward pass starting at the loss function, and fills up the `grad` properties of all `Tensor` objects connected to `L_tc` that have `requires_grad=True`. \n",
    "\n",
    "That's it! Run the next cell to run a sanity check of your [NumPy implementation](#1.A.a.-Numpy-implementation-(5-points)) by comparing its results to the PyTorch implementation. Run the cell after that one to see a plot of $\\partial L/\\partial \\beta_1$ as a function of $\\beta_1$ when $x= -2$, $y=20$, and $\\beta_0=4$ according to both your implementation and PyTorch's results. This could help you debug your solution and see that it works more generally. For example, for which $\\beta_1$ does $\\partial L / \\partial \\beta_1 (\\beta_1) = 0$? Does this make sense?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> To extract a NumPy array containing the current value of a tensor, we use the <code>.data.numpy()</code> method of that tensor.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "<b>Note:</b> If you want to run the cell below again, run the forward pass above before! Once <code>.backward()</code> is called, PyTorch frees the buffers (throws away the graph information) to reduce memory use. You can easily check this by running the cell below twice in a row: it will raise an error. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3954dbe14bed236a4167272d4a82061",
     "grade": false,
     "grade_id": "cell-d1e7ef2343760155",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Intialize partial derivatives to zero (see more on this later, by the term of zero_grad())\n",
    "try:\n",
    "    beta0_tc.grad.data.zero_()\n",
    "    beta1_tc.grad.data.zero_()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Perform the backward pass (backpropagation)\n",
    "try:\n",
    "    L_tc.backward()\n",
    "except:\n",
    "    raise Exception(\"Run the forward pass in the previous cell again if you want to evaluate \\\n",
    "the backward pass a second time (see explanations below).\")\n",
    "\n",
    "# Extract partial derivatives\n",
    "dLdbeta0_torch = np.copy(beta0_tc.grad.data.numpy())\n",
    "dLdbeta1_torch = np.copy(beta1_tc.grad.data.numpy())\n",
    "\n",
    "# Print results\n",
    "print(f\"Input x = {x_tc.data.numpy()}, Target y = {y_tc.data.numpy()}. Parameters: β₀ = {beta0_tc.data.numpy()} and β₁ = {beta1_tc.data.numpy()}.\\n\\\n",
    "Forward pass, results: L = {L_tc.data.numpy()}.\\n\\\n",
    "Backward pass, results: dL/dβ₀ = {dLdbeta0_torch} and dL/dβ₁ = {dLdbeta1_torch}.\")\n",
    "\n",
    "# Sanity check for your NumPy implementation\n",
    "_, (dLdbeta0, dLdbeta1) = model_forward_backward(-2, 20, (2, -4))\n",
    "error1 = not dLdbeta0 == dLdbeta0_torch\n",
    "error2 = not dLdbeta1 == dLdbeta1_torch\n",
    "if error1:\n",
    "    print(50*'-'+f\"\\nYour NumPy computation of dL/dβ₀ = {dLdbeta0} does not match the PyTorch result (see above). Check your code.\")\n",
    "if error2:\n",
    "    print(50*'-'+f\"\\nYour NumPy computation of dL/dβ₁ = {dLdbeta1} does not match the PyTorch result (see above). Check your code.\")\n",
    "if not error1 and not error2:\n",
    "    print(50*'-'+\"\\nCongratulations! Your NumPy computation of the gradient matches the PyTorch result for this specific setting.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7474b3d22c939a3f7b5ec82c8621836a",
     "grade": true,
     "grade_id": "cell-a2c18af04070687d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set number of points\n",
    "nrof_points = 50\n",
    "# Create tensors and generate range of beta1s to visualize\n",
    "x_tc = torch.tensor(-2.); y_tc = torch.tensor(20.); beta0_tc = torch.tensor(4., requires_grad = True)\n",
    "beta1s = np.linspace(-12, -4, nrof_points)\n",
    "# Create vectors to store results\n",
    "dLdbeta1_torch, dLdbeta1_np =  np.empty_like(beta1s), np.empty_like(beta1s)\n",
    "# Get derivatives\n",
    "for idx in range(nrof_points):\n",
    "    # Initialize tensors\n",
    "    beta1_tc = torch.tensor(beta1s[idx], requires_grad = True, dtype=torch.float64)\n",
    "    # Perform the forward pass\n",
    "    a_tc, b_tc = model_forward(x_tc, (beta0_tc, beta1_tc))\n",
    "    # Compute the loss function\n",
    "    L_tc = (b_tc - y_tc)**2\n",
    "    # Intialize partial derivatives to zero\n",
    "    try:\n",
    "        beta0_tc.grad.data.zero_(); beta1_tc.grad.data.zero_()\n",
    "    except:\n",
    "        pass\n",
    "    # Run backward pass and store result\n",
    "    L_tc.backward()\n",
    "    dLdbeta1_torch[idx] = np.copy(beta1_tc.grad.data.numpy())\n",
    "    # Run NumPy implementation\n",
    "    _, (_, dLdbeta1_np[idx]) = model_forward_backward(-2, 20, (4, beta1s[idx]))\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure(figsize = [10,5], num = r\"Derivative w.r.t. $\\beta_1$, NumPy and PyTorch implementations\")\n",
    "plt.plot(beta1s, dLdbeta1_torch, 'r-x', label = r\"$\\partial L / \\partial \\beta_1$ (PyTorch)\") \n",
    "plt.plot(beta1s, dLdbeta1_np, 'b.', label = r\"$\\partial L / \\partial \\beta_1$ (your NumPy impl.)\")\n",
    "plt.legend(); plt.xlabel(r\"$\\beta_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e2a67227414f03d9ffeec99a180c391",
     "grade": false,
     "grade_id": "cell-a9aa1280ebb2fff0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "<b>Note:</b> In PyTorch, to prevent the graph information from being cleared after calling <code>.backward()</code>, you need to set the <code>retain_graph</code> argument to <code>True</code> in your call. This is useful for calculating the gradients of specific applications (e.g., recursive neural networks (RNN)) that recursively use a module, but it is not needed in this lab. Experiment with it at your own risk!\n",
    "</div>\n",
    "\n",
    "#### Optimization algorithms in PyTorch\n",
    "\n",
    "In PyTorch, one can update the learnable parameters either by explicitly implementing an optimization algorithm using the gradients, or by relying on the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) module, which includes a wide variety of optimization algorithms to train neural networks, represented by `Optimizer` objects. The specific optimizer one selects within `torch.optim` (in the case of the cell below, stochastic gradient descent, `torch.optim.SGD`), requires as arguments\n",
    "\n",
    "* a Python list of tensors representing the parameters to be trained, and \n",
    "* other parameters, which are algorithm-dependent but almost always include the learning rate `lr`. See the cell below for an example.\n",
    "\n",
    "In order to run a step of the algorithm, and therefore update _all_ the parameters to train, one simply runs the `.step()` method of the `Optimizer` object obtained from the `torch.optim` module. This becomes extremely handy as the algorithm gets more and more involved and the number of parameters to train increases. \n",
    "\n",
    "#### A step of an optimization algorithm\n",
    "\n",
    "Run the next cell to compare a step of the manually-implemented gradient descent algorithm with `torch.optim.SGD`. Make sure to understand every line of code.\n",
    "\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "<b>Hint:</b> Note how the PyTorch optimizer has this extra word at the beginning, <i><a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">stochastic</a></i>, that we had not used before. Nevertheless, in this toy example, gradient descent and <i>stochastic</i> gradient descent are the same algorithm. Do you understand why? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tensors to initial values\n",
    "x_tc     = torch.tensor(-2, requires_grad = False, dtype=torch.float64)\n",
    "beta0_tc = torch.tensor(2,  requires_grad = True,  dtype=torch.float64)\n",
    "beta1_tc = torch.tensor(-4, requires_grad = True,  dtype=torch.float64)\n",
    "y_tc     = torch.tensor(20, requires_grad = False, dtype=torch.float64)\n",
    "\n",
    "# Perform the forward pass\n",
    "a_tc, b_tc = model_forward(x_tc, (beta0_tc, beta1_tc))\n",
    "\n",
    "# Compute the loss function\n",
    "L_tc = (b_tc - y_tc)**2\n",
    "\n",
    "# Perform backward pass\n",
    "L_tc.backward()\n",
    "\n",
    "# Update the weights manually\n",
    "beta0_tc_updated = beta0_tc - learning_rate * beta0_tc.grad\n",
    "beta1_tc_updated = beta1_tc - learning_rate * beta1_tc.grad\n",
    "\n",
    "# Extract initial values\n",
    "beta0_ini = np.copy(beta0_tc.data.numpy())\n",
    "beta1_ini = np.copy(beta1_tc.data.numpy())\n",
    "\n",
    "# Update the weights using the optimizer\n",
    "optimizer = torch.optim.SGD([beta0_tc, beta1_tc], lr=learning_rate)\n",
    "optimizer.step() \n",
    "\n",
    "print(f'Initial: β₀ = {beta0_ini}, β₁ = {beta1_ini}.')\n",
    "print(f'Updated (manual): β₀ = {beta0_tc_updated.data.numpy()}, β₁ = {beta1_tc_updated.data.numpy()}.')\n",
    "print(f'Updated (torch.optim.SGD): β₀ = {beta0_tc.data.numpy()}, β₁ = {beta1_tc.data.numpy()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6af9b67d2491db71525fe33d1aecbd47",
     "grade": false,
     "grade_id": "cell-10383a1caee619e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Unless told otherwise, PyTorch _accumulates_ gradients. This means that if the same model is run again forward and backward without first setting the `grad` of the relevant tensors to zero, these will then contain the sum of two gradients. This is done on purpose, since it can be useful, for example, to average the gradients of the network across data-target pairs in memory-limited systems, but you will not need it in this lab.\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "To make sure that the gradients are not accumulated at later iterations, one needs to clear up the <code>grad</code> property of the Tensors. This is done by any <code>Optimizer</code> object by calling the <code>.zero_grad()</code> method.\n",
    "</div>    \n",
    "\n",
    "Alternatively, if one implements the optimization algorithm manually instead, one can call the `.data.zero_()` method on the `grad` property of each tensor where the gradient needs to be put to zero, as we did at the start of the sanity check above.\n",
    "\n",
    "<div class=\" alert alert-danger\">\n",
    "    \n",
    "<b>Note:</b> Be careful to not call <code>.zero_grad()</code> before you call <code>.step()</code>! What would happen if you did? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cec9529f76dda8aaa42f3462e07c61b",
     "grade": false,
     "grade_id": "cell-85667ca2e84d9414",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Before calling optimizer.zero_grad(), dL/dβ₀ = {beta0_tc.grad.data.numpy()} and dL/dβ₁ = {beta1_tc.grad.data.numpy()}.\")\n",
    "optimizer.zero_grad()\n",
    "print(f\"After calling optimizer.zero_grad(),  dL/dβ₀ = {beta0_tc.grad.data.numpy()} and dL/dβ₁ = {beta1_tc.grad.data.numpy()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e9cb35bd0ec03f8f2f8059daeb295ca",
     "grade": false,
     "grade_id": "cell-4ef909138ba8e954",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Experimenting with optimization algorithms in PyTorch\n",
    "\n",
    "Run the next cell and check that the results are the same you obtained with your [Numpy implementation](#Gradient-descent). After that, feel free to experiment. \n",
    "\n",
    "In the next cell you will find a widget very similar to the one we used to check your implementation in NumPy, with an added dropdown menu to change the optimization algorithm. See all PyTorch optimizers [here](https://pytorch.org/docs/stable/optim.html). We didn't include *all* of them, nor used all the available parameters for each of them, but you can change the code and add any you would like to try. If you are interested, the PyTorch website has proper references to each of the optimizers. Experiment with different parameters, and feel free to change anything in the code!\n",
    "\n",
    "<div class=\" alert alert-danger\">\n",
    "    \n",
    "<b>Note:</b> The following widget is meant to only give you a taste of the different optimizers available and how simple it is to try them out in PyTorch. However, note that the simple example we test them on is not the most relevant, and much further study and testing on real problems would be needed for you to build an intution on each of them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare sliders\n",
    "gamma_value = widgets.IntSlider(value = 1, min = 1, max = 10, step = 1, description = r'$\\gamma$ val.')\n",
    "gamma_decimal = widgets.IntSlider(value = -2, min = -4, max = 4, step = 1, description = '1E')\n",
    "lr_textbox = widgets.Textarea(value = f'{10**-2:.0E}', description = r'$\\gamma =$', layout = {'width': '200px', 'height': '30px'})\n",
    "iterations_slider = widgets.IntSlider(value = 100, min = 10, max = 300, step = 10, description = 'Iterations')\n",
    "optimizer_menu = widgets.Dropdown(options = ['SGD', 'Adagrad', 'Adam', 'Adamax', 'RProp',], \n",
    "                                  value = 'SGD', description = 'Optimizer:')\n",
    "table = widgets.Textarea(value = '', layout = {'width': '55%', 'height': '300px'})\n",
    "button = widgets.Button(description = 'Optimize')\n",
    "button_res = widgets.Button(description='Clear Axes')\n",
    "# Prepare display\n",
    "box = widgets.HBox([table, widgets.VBox([gamma_value, gamma_decimal, lr_textbox, iterations_slider, optimizer_menu, button, button_res])])\n",
    "\n",
    "# Declare figure to plot evolution of parameters\n",
    "plt.close('all')\n",
    "fig = plt.figure(num = \"Convergence of PyTorch opti. algo.s\", figsize = [9,5])\n",
    "gs = fig.add_gridspec(2, 2)\n",
    "ax_loss = fig.add_subplot(121); ax_loss.set_title('Loss vs Iterations'); ax_loss.set_ylabel(\"Loss [dB]\")\n",
    "ax_b0 = fig.add_subplot(222); ax_b0.set_title(r'Parameter $\\beta_0$ vs Iterations'); ax_b0.set_ylabel(r'$\\beta_0$')\n",
    "ax_b1 = fig.add_subplot(224); ax_b1.set_title(r'Parameter $\\beta_1$ vs Iterations'); ax_b1.set_ylabel(r'$\\beta_1$')\n",
    "plt.tight_layout(); plt.show()\n",
    "display(box)\n",
    "\n",
    "# # Declare function that performs a full pass of the gradient descent algorithm\n",
    "def optimizer_full_run_pytorch(change):\n",
    "    # Set tensors to initial values\n",
    "    x_tc     = torch.tensor(-2,     requires_grad = False, dtype=torch.float64)\n",
    "    beta0_tc = torch.tensor(2, requires_grad = True,  dtype=torch.float64)\n",
    "    beta1_tc = torch.tensor(-4, requires_grad = True,  dtype=torch.float64)\n",
    "    y_tc     = torch.tensor(20,     requires_grad = False, dtype=torch.float64)\n",
    "\n",
    "    # Set learning rate, and number of iterations and printing frequency\n",
    "    learning_rate = gamma_value.value * 10 ** gamma_decimal.value\n",
    "    nrof_iterations = iterations_slider.value\n",
    "    printing_freq = 10\n",
    "    \n",
    "    # Define SGD optimizer\n",
    "    optim = optimizer_menu.value\n",
    "    if optim == 'SGD':\n",
    "        optimizer = torch.optim.SGD([beta0_tc,beta1_tc], lr=learning_rate)\n",
    "    elif optim == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad([beta0_tc,beta1_tc], lr=learning_rate)\n",
    "    elif optim == 'Adam':\n",
    "        optimizer = torch.optim.Adam([beta0_tc,beta1_tc], lr=learning_rate)\n",
    "    elif optim == 'Adamax':\n",
    "        optimizer = torch.optim.Adamax([beta0_tc,beta1_tc], lr=learning_rate)\n",
    "    elif optim == 'RProp':\n",
    "        optimizer = torch.optim.Rprop([beta0_tc,beta1_tc], lr=learning_rate)\n",
    "        \n",
    "    ## Run gradient descent and print results' table\n",
    "    # Prepare results' table\n",
    "    table_str = '|\\tIteration\\t|\\tβ₀\\t\\t\\t|\\tβ₁\\t\\t\\t|\\tPrevious loss (L_prev)\\t|\\n'+98*'-'+'\\n'\n",
    "    table_str += f'|\\t{0:3}\\t\\t|\\t{beta0:.4E}\\t|\\t{beta1:.4E}\\t|\\t-\\t\\t\\t\\t\\t|\\n'\n",
    "    # Initialize variables for plots\n",
    "    its, beta0s, beta1s, losses = np.arange(0,nrof_iterations+1), np.empty(nrof_iterations+1), np.empty(nrof_iterations+1), np.empty(nrof_iterations+1)\n",
    "    # Fill initial values\n",
    "    beta0s[0], beta1s[0] = beta0_tc.data.numpy(), beta1_tc.data.numpy()\n",
    "    # Iterate\n",
    "    for iter_num in range(nrof_iterations):\n",
    "        # Perform the forward pass\n",
    "        a_tc, b_tc = model_forward(x_tc, (beta0_tc, beta1_tc))\n",
    "        # Compute the loss function\n",
    "        L_tc = (b_tc - y_tc)**2\n",
    "        # Perform the backward pass\n",
    "        L_tc.backward()\n",
    "        # Update the parameters using the optimizer\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        # Store values and print table row\n",
    "        beta0s[iter_num+1], beta1s[iter_num+1], losses[iter_num] = beta0_tc.data.numpy(), beta1_tc.data.numpy(), L_tc.data.numpy()\n",
    "        if (iter_num+1)%printing_freq == 0:\n",
    "            table_str += f'|\\t{iter_num+1:3}\\t\\t|\\t{beta0s[iter_num+1]:.4E}\\t|\\t{beta1s[iter_num+1]:.4E}\\t|\\t{losses[iter_num]:.3E}\\t\\t\\t|\\n'\n",
    "\n",
    "        # Fill last value and close table\n",
    "        # Perform the forward pass\n",
    "        a_tc, b_tc = model_forward(x_tc, (beta0_tc, beta1_tc))\n",
    "        # Compute the loss function\n",
    "        L_tc = (b_tc - y_tc)**2\n",
    "        losses[-1] = L_tc.data.numpy()\n",
    "    table_str += 98*'-'\n",
    "    table.value = table_str\n",
    "    \n",
    "    ## Update plot\n",
    "    ax_loss.plot(np.arange(0,nrof_iterations+1), 10*np.log10(losses), label = f'{optim} (lr={learning_rate:.0E})'); ax_loss.legend() \n",
    "    ax_b0.plot(np.arange(0,nrof_iterations+1),beta0s, label = f'{optim} (lr={learning_rate:.0E})'); ax_b0.legend() \n",
    "    ax_b1.plot(np.arange(0,nrof_iterations+1),beta1s, label = f'{optim} (lr={learning_rate:.0E})'); ax_b1.legend() \n",
    "    \n",
    "def clear_axes(change):\n",
    "    ax_loss.clear(); ax_loss.set_title('Loss vs Iterations'); ax_loss.set_ylabel(\"Loss [dB]\"); ax_loss.set_xlabel(\"Iterations\");\n",
    "    ax_b0.clear(); ax_b0.set_title(r'Parameter $\\beta_0$ vs Iterations'); ax_b0.set_ylabel(r'$\\beta_0$')\n",
    "    ax_b1.clear(); ax_b1.set_title(r'Parameter $\\beta_1$ vs Iterations'); ax_b1.set_ylabel(r'$\\beta_1$')\n",
    "\n",
    "def change_lr(change):\n",
    "    learning_rate = gamma_value.value * 10 ** gamma_decimal.value\n",
    "    lr_textbox.value = f'{learning_rate:.0E}'\n",
    "\n",
    "gamma_value.observe(change_lr, 'value')\n",
    "gamma_decimal.observe(change_lr, 'value')\n",
    "# Link buttons to functions and run once\n",
    "button.on_click(optimizer_full_run_pytorch)\n",
    "button_res.on_click(clear_axes)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec75ad8e6485a959e3e8930ba171f9ae",
     "grade": false,
     "grade_id": "cell-34bb724283c9a8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "<b>Note:</b> As you may or may not have realised, the toy example we treated in this section is a one-dimensional linear regression trained through gradient descent! You only had one data-point, however. As an extra exercise, feel free to try and see if you can modify the examples to fit a one-dimensional linear regression with several data-points you generate yourself!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fbf809551214efefe953752bf463fd7",
     "grade": false,
     "grade_id": "cell-cbe72bbda4062bba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.B. Toy example: Logistic regression (6 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Let's try a slightly more complicated toy example that involves multiple composed functions, where $f(\\mathbf{x},\\mathbf{w}) = \\operatorname{\\sigma}\\left(w_0x_0 + w_1x_1+w_2\\right)$ and the loss function $L(\\mathbf{w}) = (f(\\mathbf{x},\\mathbf{w})-y)^2$. Here, you will be able to appreciate even more the convenience of frameworks like PyTorch. The corresponding computational graph would be as below.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <p align=\"center\" style=\"padding: 10px\">\n",
    "            <img src=\"images/toy-example_logistic-regression.png\" alt=\"Drawing\" style=\"width: 500px;\"/><br>\n",
    "            <em style=\"color: grey\">Computational graph of the network computing the logistic regression. $a$, $b$, $c$ and $o$ are highlighted points in the graph for your later use in deriving the backward pass.</em>\n",
    "        </p> \n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Here, $\\mathbf{x} = [x_0,x_1]^\\mathrm{T}$ is a two-dimensional input, $y$ denotes the desired output (label or target), and our goal is to find the parameter vector $\\mathbf{w} = [w_0,w_1,w_2]^\\mathrm{T}$ that minimizes the loss function. $\\operatorname{\\sigma}$ represents the sigmoid function, which is often used as a nonlinear activation function in neural networks, and is defined as\n",
    "\n",
    "$$\\operatorname{\\sigma}(x) = \\frac{1}{1+e^{-x}}\\,.$$\n",
    "\n",
    "In this section you will first define this more complex neural network in NumPy, and then we will do it in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da81ea2dcbb2d2fcff69ca08fb56d871",
     "grade": false,
     "grade_id": "cell-33fdafe3dee766ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.a. Numpy implementation (4 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "In this new toy example, you will be able to appreciate the modularity of backpropagation, in which the computation of the overall gradient is split in small forward and backward rutines for each function used in a network, all composed in the proper order. \n",
    "\n",
    "From the computational graph above, it is clear that we will benefit greatly from having characterized the activation function $\\operatorname{\\sigma}(\\cdot)$ in this way, so that we do not have to explicitly deal with the interaction between the derivatives of $L$ and those of $\\sigma$. As we learned in the previous example, then, we will first build the **computational graph of the sigmoid function below**.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <p align=\"center\" style=\"padding: 10px\">\n",
    "            <img src=\"images/sigmoid.png\" alt=\"Drawing\" style=\"width: 500px;\"/><br>\n",
    "            <em style=\"color: grey\">Computational graph of the sigmoid activation function $\\operatorname{\\sigma}(x)$. As seen in the definition of $\\operatorname{\\sigma}(x)$, $x$ is first multiplied by $-1$ to get $a$, then used as exponent of $e$ to get $b$, added $1$ to get $c$, and taken the reciprocal to get $o$. $a$, $b$, $c$ and $o$ are highlighted points in the graph for your later use in deriving the backward pass, different of those highlighted in the previous computational graphs. </em>\n",
    "        </p> \n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "As a warm up exercise, and for **1 point, complete the backward pass through the computational graph of the sigmoid function** in the function `model_forward_backward_sigmoid` below so that it returns the partial derivatives\n",
    "* `dodc`: $\\partial o / \\partial c$, \n",
    "* `dodb`: $\\partial o / \\partial b$, \n",
    "* `dbda`: $\\partial b / \\partial a$, and \n",
    "* `dodx`: $\\partial o / \\partial x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9c5622c678e28f99cd6ca5d103baa0d",
     "grade": false,
     "grade_id": "cell-c82ebf5c2458d0d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Functions that implement the forward and backward passes of the sigmoid activation function\n",
    "# Forward pass (using notation of the highlighted points in the graph above)\n",
    "def model_forward_sigmoid(x):\n",
    "    a = -x\n",
    "    b = np.exp(np.copy(a))\n",
    "    c = 1 + b\n",
    "    o = 1/c\n",
    "    return a, b, c, o\n",
    "\n",
    "# Backward pass\n",
    "def model_forward_backward_sigmoid(x):\n",
    "    # Initialize variables (make sure to change all their values)\n",
    "    dodc = None\n",
    "    dodb = None\n",
    "    doda = None\n",
    "    dodx = None\n",
    "    \n",
    "    # Perform the forward pass (get a, b, c, and o in the graph above)\n",
    "    a, b, c, o = model_forward_sigmoid(x)\n",
    "    \n",
    "    # Perform the backward pass (fill dodx using the chain rule within the computational graph above)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return o, dodc, dodb, dbda, dodx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd2bbb65c4c45159861d82094c4debe7",
     "grade": false,
     "grade_id": "cell-a9b0c873956ba8f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to see the graphs of $\\operatorname{\\sigma}(x)$ and $\\partial \\sigma/\\partial x (x)$ according to your implementation of `model_forward_backward_sigmoid`. We will calculate these values for an array in the range $[-10, 10]$, to look at all the regions of interest. Verify that your results make sense! If in doubt, go ahead and look for the correct plots online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41ea25cdb32dad25b6dc0f6adea5bc34",
     "grade": true,
     "grade_id": "cell-91af9ff26c7b12d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " # Plot the results of your model_forward_backward_sigmoid function\n",
    "# Generate axis\n",
    "x = np.linspace(start=-10, stop=10, num=100)\n",
    "# Get results\n",
    "o, dodc, dodb, dbda, dodx = model_forward_backward_sigmoid(x)\n",
    "\n",
    "# Plot\n",
    "plt.close(\"all\")\n",
    "plt.figure(figsize=[10,5], num = r\"$\\operatorname{\\sigma}(x)$ and $(\\partial \\sigma/\\partial x)(x)$\")\n",
    "plt.subplot(211); plt.plot(x,o); plt.ylabel(r\"$\\operatorname{\\sigma}(x)$\"); plt.grid(); plt.title('Sigmoid function')\n",
    "plt.subplot(212); plt.plot(x,dodx); plt.ylabel(r\"$(\\partial \\sigma / \\partial x) (x)$\"); plt.xlabel(r\"$x$\"); plt.grid()\n",
    "plt.title('Derivative of the sigmoid function'); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7ef005df3f3efb10e39016b84a7f416",
     "grade": false,
     "grade_id": "cell-2f1510aad520d1b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Although backpropagation is very advantageous when expressions can not be derived by hand, it has a cost in terms of memory and computation (and without libraries performing it for you, it can be long to implement as you have already seen). In cases in which the derivative is either well known or easy to derive, it will be beneficial to exploit it directly to obtain a more efficient implementation. \n",
    "\n",
    "The sigmoid function has in fact a known, closed-form expression for $\\partial \\sigma/\\partial x$, i.e. `dodx`. In particular, we simply derive\n",
    "\n",
    "$$\\left(\\frac{\\partial\\sigma}{\\partial x}\\right)(x) = \\frac{\\partial}{\\partial x}\\left\\lbrace \\frac{1}{1+e^{-x}}\\right\\rbrace = \\frac{e^{-x}}{(1+e^{-x})^2} =  \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right) \n",
    "= \\left[ 1 - \\operatorname{\\sigma}(x) \\right] \\operatorname{\\sigma}(x)\\,.$$\n",
    "\n",
    "\n",
    "For **1 point**, use this result to complete the `simple_forward_backward_sigmoid` function below and ensure it matches the results of `model_forward_backward_sigmoid`. After completing it, run the cell below for a sanity check that will check both that the results match (at least partially) and that the simple implementation is faster.\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "    \n",
    "<b>Hint:</b> Use the given <code>sigmoid</code> function to calculate $\\operatorname{\\sigma}(x)$.\n",
    "</div>\n",
    "<div class = 'alert alert-danger'>\n",
    "    \n",
    "<b>Note:</b> Of course, you cannot call <code>model_forward_backward_sigmoid</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "422260f0280b1522ca5ca72b5530305b",
     "grade": false,
     "grade_id": "cell-05abf1d64839d8e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def simple_forward_backward_sigmoid(x):\n",
    "    # Initialize o (to store forward pass)\n",
    "    o = None\n",
    "    # And dodx (to compute gradient)\n",
    "    dodx = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return o, dodx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac721230570d039745b0ce41ebacf3a",
     "grade": true,
     "grade_id": "cell-f1ec807e58bdb249",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the time library to check efficiency\n",
    "import time\n",
    "\n",
    "# Run a sanity check at hand-picked points\n",
    "error_value = False\n",
    "x = np.array([-5,0,5])\n",
    "_, _, _, _, dodx_backprop = model_forward_backward_sigmoid(x)\n",
    "_, dodx_simple = simple_forward_backward_sigmoid(x)\n",
    "\n",
    "if not np.allclose(dodx_backprop, dodx_simple):\n",
    "    print(f\"Your two implementations do not seem to match. Revise your code. Using backpropagation,\\ndodx = {dodx_backprop},\\nwhile \\\n",
    "using the simple approach,\\ndodx = {dodx_simple}.\\n\")\n",
    "    error_value = True\n",
    "\n",
    "# Compare timing using an array of nrof_points\n",
    "error_time = False\n",
    "nrof_reps = 1000; nrof_points = 1000\n",
    "x = np.linspace(start=-10, stop=10, num=nrof_points)\n",
    "\n",
    "start = time.time()\n",
    "for ind in range(nrof_reps):\n",
    "    model_forward_backward_sigmoid(x)\n",
    "time_backprop = (time.time() - start)/nrof_reps\n",
    "start = time.time()\n",
    "for ind in range(nrof_reps):\n",
    "    simple_forward_backward_sigmoid(x)\n",
    "time_simple = (time.time() - start)/nrof_reps\n",
    "\n",
    "ratio = time_backprop / time_simple\n",
    "\n",
    "if ratio < 1:\n",
    "    print(\"The sanity check did not pass. Revise your code to ensure the simple implementation is faster than applying the chain rule.\")\n",
    "    error_time = True\n",
    "\n",
    "if not error_value and not error_time:\n",
    "    print(\"Great, your code passed the sanity check! simple_forward_backward_sigmoid and model_forward_backward_sigmoid seem to agree, and\", end=\" \")\n",
    "    print(f\"simple_forward_backward_sigmoid is {ratio:.3} times faster than model_forward_backward_sigmoid.\\n\\\n",
    "Average time_backprop \\t= {time_backprop:.4E} s, \\nAverage time_simple \\t= {time_simple:.4E} s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07279ca3f2fc07dddd6344ce4ecb39af",
     "grade": false,
     "grade_id": "cell-cc01b97e987e7003",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, **for 2 points**, complete a full backward chain of the toy network presented at the beginning of [this section](#1.B.a.-Numpy-implementation-(4-points)). This time, we will consider a variable number of input-target pairs `B`. For that, you will complete the function `model_forward_backward_logistic` below, which takes as parameters\n",
    "* `x`: `B` input arrays $\\mathbf{x}$, all in a NumPy array of shape `(B,2)`,\n",
    "* `y`: `B` labels or targets, in a NumPy array of shape `(B,)`,\n",
    "* `w`: the parameters' array, $\\mathbf{w} = [w_0,w_1,w_2]^\\mathrm{T}$, as a NumPy array of size `(3,)`,\n",
    "\n",
    "and returns the `B` values of the loss function `L` in a NumPy array of shape `(B,)` and gradients of the loss function with respect to the trainable parameters in a NumPy array of shape `(B,3)`. \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "    \n",
    "<b>Hints:</b> \n",
    "<ul><li>Use the <code>simple_forward_backward_sigmoid</code> function you prepared above.</li><li>For your code, do not worry much about <code>B</code>: a naïve implementation will work for any <code>B</code>, and we have handled input/output issues for you.</li></ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cf2788ad27057b9f47ccee71de9c42a",
     "grade": false,
     "grade_id": "cell-b6782bc26d428e5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# perform the forward pass and backward pass (backpropagation).\n",
    "def model_forward_backward_logistic(x, y, w):    \n",
    "    \n",
    "    # Extract inputs and weights\n",
    "    x0, x1 = x[:,0], x[:,1]\n",
    "    w0, w1, w2 = w\n",
    "    \n",
    "    # Initialize variables (make sure to change all their values)\n",
    "    o, L, dLdw0, dLdw1, dLdw2 = None, None, None, None, None\n",
    "    \n",
    "    ## Perform the forward pass and start the backward one\n",
    "    # Compute a, b, c, o, and dodc\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute the loss function\n",
    "    # Place the right value in the variable L\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Perform the backward pass \n",
    "    # Place the right values in the variables dLdw0, dLdw1, and dLdw2\n",
    "    \n",
    "    # Backpropagate through L (compute dLdo)    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Backpropagate through the sigmoid function (compute dLdc)  \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Backpropagate through c (compute dLda, dLdb and dLdw2)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Backpropagate through a to compute dLdw0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "    # Backpropagate through b to compute dLdw1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Return predictions, loss, and gradients\n",
    "    return o, L, np.hstack((np.expand_dims(dLdw0, axis=1), np.expand_dims(dLdw1, axis=1), np.expand_dims(dLdw2, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a685db46865f6a59800a30adbe1d0f8",
     "grade": false,
     "grade_id": "cell-61deacf4b0a05d55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let us apply your function to a toy dataset, $200$ points sampled from Gaussian distributions ($100$ per distribution) with $\\sigma=0.25$, and with means of $[1, 1]$ and $[-1, -1]$. As you can see, there is barely any overlap, and the optimal decision boundary should be reasonably close to the linear function $y = -x$.\n",
    "\n",
    "Run the following cell to declare and visualize this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2bf76c45b3e67e0ed07d83dc63d9c79",
     "grade": false,
     "grade_id": "cell-d8545c6fdebe051b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create two classes of 2D data-points\n",
    "# Set the number of points per class\n",
    "nrof_points_per_class = 100\n",
    "# Generate the random points of both classes\n",
    "x_class_0 = np.random.multivariate_normal(mean=[1,1],   cov=1/4*np.eye(2), size=(nrof_points_per_class,))\n",
    "x_class_1 = np.random.multivariate_normal(mean=[-1,-1], cov=1/4*np.eye(2), size=(nrof_points_per_class,))\n",
    "# Visualize the database\n",
    "plt.close(\"all\")\n",
    "plt.scatter(x_class_0[:,0],x_class_0[:,1], color='blue', label = r'Class 0 ($\\mu = [1, 1]$)')\n",
    "plt.scatter(x_class_1[:,0],x_class_1[:,1], color='red', label = r'Class 1 ($\\mu = [-1, -1]$)')\n",
    "plt.grid(); plt.title('2D Toy Dataset'); plt.xlabel('x'); plt.ylabel('y'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d28299a41c8a286c8116cec436252f6",
     "grade": false,
     "grade_id": "cell-b6ba46f4a83b4c9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the following cell to see the evolution of the parameters of your network when running SGD with batch size $10$ and learning rate $10^{-2}$ for $1000$ epochs.\n",
    "\n",
    "Read the code in more detail if you are interested by an example on how one can implement SGD manually, handle batch sizes while training, understand the concept of epochs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a6b4d4988eda5916678a40d27ae9102",
     "grade": true,
     "grade_id": "cell-36bf4374ba5dc7e0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize x and y\n",
    "x = np.empty((2*nrof_points_per_class,2)); y = np.empty((2*nrof_points_per_class,))\n",
    "# Initialize grid to evaluate decision boundary\n",
    "xv, yv = np.meshgrid(np.linspace(-2.5, 2.5, 200), np.linspace(-2, 2.5, 200))\n",
    "X_pred = np.concatenate([xv.reshape(-1, 1), yv.reshape(-1, 1)], axis=1)\n",
    "Y_pred = np.zeros((X_pred.shape[0],))\n",
    "decision_boundary = []\n",
    "# Get a random permutation of the indeces\n",
    "perm = np.random.permutation(2*nrof_points_per_class)\n",
    "# Use it to mix the two classes well\n",
    "x[perm[:nrof_points_per_class],:] = x_class_0; y[perm[:nrof_points_per_class]] = np.zeros((nrof_points_per_class,));\n",
    "x[perm[nrof_points_per_class:],:] = x_class_1; y[perm[nrof_points_per_class:]] = np.ones( (nrof_points_per_class,));\n",
    "\n",
    "## Configure parameters\n",
    "# Set initial parameter values\n",
    "w = np.array([0, 1, 2]) \n",
    "# Set learning rate\n",
    "learning_rate = 1e-2\n",
    "# Set the number of epochs\n",
    "nrof_epochs = 1000\n",
    "# Set the batch size\n",
    "batch_size = 10\n",
    "# Deduce the number of iterations needed\n",
    "nrof_iterations = np.ceil( 2*nrof_points_per_class*nrof_epochs/batch_size ).astype(np.int)\n",
    "# Set printing frequency (approx. every 50 epochs)\n",
    "printing_freq = 50*np.ceil( 2*nrof_points_per_class/batch_size ).astype(np.int)\n",
    "\n",
    "print(f\"Running {nrof_iterations} iterations of SGD with batch_size = {batch_size}:\\n\")\n",
    "\n",
    "## Run stochastic gradient descent and print results' table\n",
    "# Prepare results' table\n",
    "print('| Iteration |\\tw₀\\t|\\tw₁\\t|\\tw₂\\t|\\tLoss\\t|\\n'+73*'-')\n",
    "# Initialize variables for plots\n",
    "its, ws, batch_losses = np.arange(0,nrof_iterations+1), np.empty((nrof_iterations+1, 3)), np.empty(nrof_iterations)\n",
    "losses = []; its_stored = []\n",
    "# Fill initial value\n",
    "ws[0,:] = w\n",
    "# Iterate\n",
    "for iter_num in range(nrof_iterations):\n",
    "    # Extract corresponding batch\n",
    "    indices = np.arange((iter_num-1)*batch_size, iter_num*batch_size)\n",
    "    x_batch = x.take(indices = indices, mode = 'wrap', axis = 0)\n",
    "    y_batch = y.take(indices = indices, mode = 'wrap', axis = 0)\n",
    "    # Make a forward - backward pass\n",
    "    _, L_prev, gradient = model_forward_backward_logistic(x_batch, y_batch, w)\n",
    "    # Average the losses and gradients over the batch\n",
    "    L_prev   = np.mean(L_prev,   axis=0)\n",
    "    gradient = np.mean(gradient, axis=0)\n",
    "    # Take the gradient descent step\n",
    "    w = w - learning_rate * gradient\n",
    "    # Store variables and inform of progress\n",
    "    ws[iter_num+1,:], batch_losses[iter_num] = w, L_prev \n",
    "    if (iter_num)%printing_freq == 0:\n",
    "        # Another pass just to evaluate decision boundary\n",
    "        prediction_probs, _, _ = model_forward_backward_logistic(X_pred, Y_pred, w)\n",
    "        decision_boundary.append(prediction_probs.reshape(xv.shape))\n",
    "        its_stored.append(iter_num+1); losses.append(np.mean( model_forward_backward_logistic(x, y, w)[1], axis=0 ))\n",
    "        print(f'| {iter_num+1:9} | {w[0]:+.2E} |   {w[1]:+.2E}   |   {w[2]:+.2E}   |   {losses[-1]:+.2E}   |')\n",
    "# Fill last value and close table\n",
    "print(73*'-')\n",
    "\n",
    "## Plot results\n",
    "plt.close(\"all\")\n",
    "plt.figure(num = \"Convergence of SGD - Logistic\", figsize = [10,5])\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(0,nrof_iterations), 10*np.log10(batch_losses), label = \"Batch loss\")\n",
    "plt.plot(its_stored, 10*np.log10(losses), 'ro-', label = \"Average loss\")\n",
    "plt.legend(); plt.xlabel(\"Iterations\"); plt.ylabel(\"Loss function [dB]\")\n",
    "plt.subplot(322)\n",
    "plt.plot(np.arange(0,nrof_iterations+1),ws[:,0]); plt.ylabel(r\"$w_0$\")\n",
    "plt.subplot(324)\n",
    "plt.plot(np.arange(0,nrof_iterations+1),ws[:,1]); plt.ylabel(r\"$w_1$\")\n",
    "plt.subplot(326)\n",
    "plt.plot(np.arange(0,nrof_iterations+1),ws[:,2]); plt.ylabel(r\"$w_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b12d0967a7b7c9538a98fd070245798d",
     "grade": false,
     "grade_id": "cell-09e9f0719683e757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By looking at the previous graphs, what do you think about the optimization of the parameters? It is a bit hard to make a judgment. Run the next cell to look at the evolution of the output of the network and judge by yourself! Network outputs close to $0$ are displayed in blue, while those close to $1$ are displayed in yellow. \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "<b>Note:</b> Click the button <code>play</code> to see the evolution of the network as an animation! Alternatively, move the slider to choose the iteration that you want to visualize.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5568dcab334ededc3c5d0484464b0696",
     "grade": false,
     "grade_id": "cell-eb3c82a68ef4ef5f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Callback function for the play widget\n",
    "def play_func(value):\n",
    "    i = int(value['new']/10000)\n",
    "    ax = plt.gca()\n",
    "    ax.clear()\n",
    "    # Display decision boundary\n",
    "    im = ax.imshow(decision_boundary[i], origin='lower', extent=(-2.5, 2.5, -2.5, 2.5), vmin=0, vmax=1)\n",
    "    # Plot the dataset points\n",
    "    plt.scatter(x_class_0[:,0],x_class_0[:,1], color='blue', label = r'Class 0 ($\\mu = [1, 1]$)')\n",
    "    plt.scatter(x_class_1[:,0],x_class_1[:,1], color='red', label = r'Class 1 ($\\mu = [-1, -1]$)')\n",
    "    ax.set_xlim([-2.5, 2.5])\n",
    "    ax.set_ylim([-2.5, 2.5])\n",
    "    ax.set_title(f'Decision boundary at iteration {i*10000+1}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1)\n",
    "# Initialize the figure\n",
    "play_func({'new':0})\n",
    "plt.show()\n",
    "# Create the play widget\n",
    "play = widgets.Play(value=0, min=0, max=(len(decision_boundary)-1)*10000, step=10000, interval=1000, description=\"Press play\")\n",
    "# Add callback function\n",
    "play.observe(play_func, names='value')\n",
    "# Add slider\n",
    "slider = widgets.IntSlider(min=0, max=(len(decision_boundary)-1)*10000, step=10000)\n",
    "widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "# Display\n",
    "widgets.HBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9691561c446ee8bf05ac789e9ea740c2",
     "grade": false,
     "grade_id": "cell-1886d0eabfa25c21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.b. PyTorch implementation (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "As we introduced in the previous toy example, we can implement the same computation graph using PyTorch. For **2 points**, complete the function `model_forward_backward_logistic_pytorch`, that computes the forward pass and the backward pass in PyTorch. It takes as parameters\n",
    "\n",
    "* `x`: the datapoints as an array of shape `(B, 2)`,\n",
    "* `y`: the class to which each datapoint belongs to (the label) as an array of shape `(B,)`,\n",
    "* `w`: the parameters of the network as an array of shape `(3,)`.\n",
    "\n",
    "Just as your function `model_forward_backward_logistic`, it returns \n",
    "\n",
    "* `L`: the value of the loss function,\n",
    "* `(dLdw0, dLdw1, dLdw2)`: the gradient of the loss with respect to each of the parameters.\n",
    "\n",
    "\n",
    "<div class = 'alert alert-success'>\n",
    "<b>Hints</b>: In the next cell, you will have to:\n",
    "<ul>\n",
    "<li>Define the variables as PyTorch Tensors. Remember that you can use the command <code>torch.tensor</code>, and <b>don't forget about the gradient requirements!</b></li>\n",
    "<li>Perform, in vectorized fashion, the appropriate computations. This step should not be very different than if you were doing it with NumPy. <ul><li><b><i>Hint: Use </i><a href=\"https://pytorch.org/docs/stable/generated/torch.sigmoid.html\"><code>torch.sigmoid()</code></a><i> function to calculate $\\sigma(\\cdot)$</i></b>.</li></ul></li>\n",
    "<li>Compute the loss. <b>PyTorch does not let you compute the loss of an array, so you will need the mean of the losses across the <code>B</code> data pairs</b>. <ul><li><i>Hint: you can use the method </i><a href=\"https://pytorch.org/docs/stable/generated/torch.mean.html\"><code>.mean()</code></a> on any <code>Tensor</code>.</li></ul></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9641da3f6757451440b4b067a90d6e2",
     "grade": false,
     "grade_id": "cell-836f8e27c36e955d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the inputs into Tensors using `torch.tensor`\n",
    "def model_forward_backward_logistic_pytorch(x, y, w):\n",
    "    # Initialize return variables\n",
    "    L_tc = dLdw0_tc = dLdw1_tc = dLdw2_tc = None\n",
    "    \n",
    "    # Variable names (tensors) that you will need to define: \n",
    "    # x0_tc, x1_tc, w0_tc, w1_tc, w2_tc, y_tc\n",
    "    \n",
    "    # x0_tc and x1_tc are given as an example\n",
    "    x0_tc = torch.tensor(x[:, 0], requires_grad = False, dtype=torch.float64)\n",
    "    x1_tc = torch.tensor(x[:, 1], requires_grad = False, dtype=torch.float64)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Perform the forward pass - relevant operations with the tensors you just defined\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Compute the loss\n",
    "    # Variable names to define: L_tc\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Perform the backward pass\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Hint: You don't need to implement any derivative yourself, if you\n",
    "    # did everything correctly, the tensors already contain it\n",
    "    dLdw0_tc = w0_tc.grad.data\n",
    "    dLdw1_tc = w1_tc.grad.data\n",
    "    dLdw2_tc = w2_tc.grad.data\n",
    "    \n",
    "    return L_tc, (dLdw0_tc, dLdw1_tc, dLdw2_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61acca91ed0c31cfc041543a76914535",
     "grade": false,
     "grade_id": "cell-0f62357f97ac11a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Does the output match with the value that you calculated previously with Numpy? Run the next cell to find out. \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "Note that your function <code>model_forward_backward_logistic</code> (NumPy) should return arrays of the same size as the originals, while <code>model_forward_backward_logistic_pytorch</code> returns their means. Thus, we will get the mean over the NumPy arrays and test against the results in PyTorch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e62583e402b6e58de0a7d3c2658c6c4a",
     "grade": true,
     "grade_id": "cell-db3e49d280be1e31",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First of all, lets reset the weights\n",
    "w = np.array([0.5, 0.5, 0.5]) \n",
    "_, L_prev_np, gradient_np = model_forward_backward_logistic(x, y, w)\n",
    "L_prev_tc, (dLdw0_tc, dLdw1_tc, dLdw2_tc) = model_forward_backward_logistic_pytorch(x, y, w)\n",
    "\n",
    "if np.isclose(L_prev_np.mean(), L_prev_tc.data.numpy()):\n",
    "    print('Well done! Your NumPy and PyTorch implementations compute the same loss.')\n",
    "else:\n",
    "    print('Your NumPy and PyTorch implementations do not compute the same loss, check your functions again!\\\n",
    "           Use the decision boundary visualization as a reference.')\n",
    "\n",
    "if np.isclose(gradient_np[:, 0].mean(), dLdw0_tc.data.numpy()):\n",
    "    print('Well done! Your NumPy and PyTorch implementations compute the same dLdw0.')\n",
    "else:\n",
    "    print('Your NumPy and PyTorch implementations do not compute the same dLdw0, check your functions again!\\\n",
    "           Use the decision boundary visualization as a reference.')\n",
    "    \n",
    "if np.isclose(gradient_np[:, 1].mean(), dLdw1_tc.data.numpy()):\n",
    "    print('Well done! Your NumPy and PyTorch implementations compute the same dLdw1.')\n",
    "else:\n",
    "    print('Your NumPy and PyTorch implementations do not compute the same dLdw1, check your functions again!\\\n",
    "           Use the decision boundary visualization as a reference.')    \n",
    "    \n",
    "if np.isclose(gradient_np[:, 2].mean(), dLdw2_tc.data.numpy()):\n",
    "    print('Well done! Your NumPy and PyTorch implementations compute the same dLdw2.')\n",
    "else:\n",
    "    print('Your NumPy and PyTorch implementations do not compute the same dLdw2, check your functions again!\\\n",
    "           Use the decision boundary visualization as a reference.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3acc49684656420215c26083364e1978",
     "grade": false,
     "grade_id": "cell-3128558af474cfb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.c. PyTorch implementation with higher abstractions\n",
    "[Back to Index](#Index)\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives. For large neural networks, however, raw autograd can be a bit too low-level. For example, what if the input dimension goes beyond two, like an image of size $256\\times 256$? \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "<b>Note:</b> While throughout the course we have been treating images as $2$-dimensional objects, from a machine learning perspective each pixel corresponds to one dimension. A <i>standard</i> mobile phone has a camera resolution of 12 MP! Of course, these images are colored, so that they have 36 million dimensions! \n",
    "</div>\n",
    "\n",
    "What if you want to change the loss function from the mean square error to the binary cross-entropy loss? What if you want to change the activation function to others such as [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)? What if you want to use [convolutional layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)?, or some new funny architectures? It would be highly inconvenient to build your model implementing all the forward and backward operations as above. \n",
    "\n",
    "In this example, we will show you how to use PyTorch packages for implementing our toy problem\n",
    "\n",
    "$$f(\\mathbf{x},\\mathbf{w}) = \\sigma\\left((w_0\\,x_0 + w_1\\,x_1)+w_2\\right)\\mbox{, with }L(\\mathbf{w}) = (f(\\mathbf{x},\\mathbf{w})-y)^2.$$\n",
    "\n",
    "From now on, we will not consider each dimension of the inputs in the computational graph separately: instead, we can do the vector calculus with `autograd`. To do so, run the next cell to redefine our variables to a vector form. \n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "<b>Note:</b> We split $\\mathbf{w} = [w_0,w_1,w_2]^\\mathrm{T}$ in the \"weights\", $w_0$ and $w_1$, and the \"bias\" terms, $w_2$. These are the common names used in machine learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09200031c9315f1d0bc26e77087956b3",
     "grade": false,
     "grade_id": "cell-210b1d049918fcb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set some inputs\n",
    "x_tc      = torch.tensor(x, requires_grad = False, dtype=torch.float64)\n",
    "weight_tc = torch.tensor(w[:2], requires_grad = True, dtype=torch.float64)\n",
    "bias_tc   = torch.tensor(w[2], requires_grad = True, dtype=torch.float64)\n",
    "y_tc      = torch.tensor(y, requires_grad = False, dtype=torch.float64)\n",
    "print(f'x_tc shape: {x_tc.size()}\\nweight_tc shape: {weight_tc.size()}\\nbias_tc shape: {bias_tc.size()}\\ny_tc shape: {y_tc.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15d271a53157c21ea79e24bacc7eaadc",
     "grade": false,
     "grade_id": "cell-a2473f1afb4ef3f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### [`torch.nn`](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "In PyTorch, the `torch.nn` (full documentation [here](https://pytorch.org/docs/stable/nn.html)) package provides higher-level abstractions over raw computational graphs that are useful for building neural networks. The `nn` package defines a set of `Modules`, which are roughly equivalent to neural network layers. A `Module` receives input Tensors and computes output Tensors, but may also hold an internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) that are commonly used when training neural networks.\n",
    "\n",
    "Here, we will use the `nn` package to define our model as a sequence of layers. In brief [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) (one of the modules you will use the most in your deep learning journey) is a module which contains other modules, and applies them in sequence to produce its output.\n",
    "\n",
    "<div class = 'alert alert-success'>\n",
    "So yes, as you have guessed, it is possible to define and train huge neural networks, with millions of parameters in relatively few lines of code. We are sorry for making you implement everything manually, but we hope that you feel entirely comfortable with the concepts of backpropagation and optimization. That will certainly help you follow the next parts of the lab.  \n",
    "</div>\n",
    "\n",
    "#### Define a model\n",
    "In our case, we will use `torch.nn.Linear` and `torch.nn.Sigmoid` modules. The [Linear module](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) computes output from input using a linear function, and holds internal Tensors for its `weight` and `bias`. The [Sigmoid module](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid) applies the sigmoid function to the output of the previous Linear module. Run the next cell to define your Sequential model with one Linear layer (with 2 inputs and 1 output) and one Sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Sequential object\n",
    "model = torch.nn.Sequential(\n",
    "    # Declare Linear layer with 2 inputs and one output\n",
    "    torch.nn.Linear(in_features=2, out_features=1, bias=True),\n",
    "    # Declare sigmoid function\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Cast the parameters of the model in float64 format to match it with the input format.\n",
    "model.double() \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4cbfef87c18c14e9d80c5fff7dfa42ce",
     "grade": false,
     "grade_id": "cell-eb4c24a0ba5d1afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, in the `Sequential` module we have included two further modules, `Linear` and `Sigmoid`. You can call each module of your model by *normal* indexing; in the last cell, `model` is an iterable where the $i^{\\mathrm{th}}$ element is the $i^{\\mathrm{th}}$ module defined. For example, the first module of our model is a `Linear` module which takes a two-dimensional input and outputs a one-dimensional feature. You can call and even set its `weight` and `bias` with specific values. \n",
    "\n",
    "Run the next cell and do some exploration on `model`! Take inspiration from the few lines that we already wrote.\n",
    "\n",
    "<div class = 'alert alert-success'>\n",
    "If you took <a href=\"https://moodle.epfl.ch/enrol/index.php?id=522\" >IP1</a>, you probably feel comfortable with manipulating NumPy Arrays. \n",
    "    \n",
    "Maybe now is the time to start exploring Tensor methods (class reference [here](https://pytorch.org/docs/master/tensors.html#tensor-class-reference)). There are a lot of operations you can apply to a Tensor, e.g. <code>t = t.add(1)</code> will add $1$ to the tensor <code>t</code> (with <code>t.add_(1)</code> being the in-place version, meaning that there is no need to reassign the value). Almost any operation you can think of (e.g., absolute value, exponential) can be applied this way. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some exploration\n",
    "print('Linear layer: ', model[0]) # Try print(model[1])\n",
    "print('\\nInitial weights: ', model[0].weight) # Try print(model[1])\n",
    "print('\\nInitial bias: ', model[0].bias) # Try print(model[1])\n",
    "\n",
    "# Now let's try to modify the bias. First we set it to 1\n",
    "model[0].bias = torch.nn.Parameter(torch.empty((1,)).fill_(1))\n",
    "print('\\nNew bias: ', model[0].bias) # Try print(model[1])\n",
    "\n",
    "# Now let's set the values of the variable bias = w2\n",
    "model[0].bias.data = bias_tc\n",
    "\n",
    "# Great! Keep on with the exploration!\n",
    "\n",
    "# Try print(model[1])\n",
    "\n",
    "# Try to set the values of the variables weight = weight_tc = (w0, w1)\n",
    "\n",
    "print('\\nNew weights: ', model[0].weight) # Try print(model[1])\n",
    "print('\\nNew bias: ', model[0].bias) # Try print(model[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9b94e4fb29a50f439a091b82b43413e",
     "grade": false,
     "grade_id": "cell-fcd3ac02bba75a04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The forward pass\n",
    "\n",
    "The forward pass is now straighforward: You can simply give the input as an argument to the model. You can even separately access to the intermediate layers and their outputs.\n",
    "\n",
    "Run the next cell, and go through every line on it to see how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12551dbfd0643d19bfa1b81735f700dc",
     "grade": false,
     "grade_id": "cell-34b3b6aac6ef6a73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "output_1st_layer = model[0](x_tc)\n",
    "output_2nd_layer = model[1](output_1st_layer)\n",
    "\n",
    "# 2\n",
    "output = model(x_tc)\n",
    "\n",
    "# Comparison (print the number of elements different from zero in the array #1 - #2)\n",
    "np.count_nonzero(output_2nd_layer.data.numpy() - output.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e902c2d9e94929532bcb7221037036",
     "grade": false,
     "grade_id": "cell-59bd9422afeb9a94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Define different loss functions\n",
    "\n",
    "It is also very easy to set a loss function using `torch.nn`. PyTorch provides a list of loss functions that are frequently used for training neural networks, you can check the reference [here](https://pytorch.org/docs/stable/nn.html#loss-functions). There, you will also find further documentation on the different functions.\n",
    "\n",
    "Run the next cell to compute the `Mean Squared Error`, the loss we have been using, using `torch.nn` and perform the backward pass on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88eea18d718ec95eb86f813136fa58ed",
     "grade": false,
     "grade_id": "cell-c7b06be847b011f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Compute the loss (flatten to reduce the extra dimension)\n",
    "L_tc = loss_fn(output.flatten(), y_tc)\n",
    "\n",
    "# Perform the backward pass\n",
    "try:\n",
    "    L_tc.backward()\n",
    "except:\n",
    "    raise Exception(\"WARNING:\\nRun the forward pass in the previous cell again if you want to evaluate \\\n",
    "the backward pass a second time.\")\n",
    "    \n",
    "print(L_tc.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2701c1837c04b0507bcb710fe5c08701",
     "grade": false,
     "grade_id": "cell-d5c35e4e909ab73e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have ran `L_tc.backwards`, let's look at the gradients again. They are now multi-dimensional tensors, as they should!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d337615b84682f0e4f652dd155284437",
     "grade": false,
     "grade_id": "cell-0b3a75bc140f8591",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('[PyTorch results] dL/dw₀: %.4f, dL/dw₁: %.4f, dL/dw₂: %.4f, L: %.4f \\n' % (model[0].weight.grad.data[0][0], model[0].weight.grad.data[0][1], model[0].bias.grad.data,L_tc.data))\n",
    "print('Note that now your gradient is also multi-dimensional: \\ndL/dw: %s \\ndL/db: %s'% (model[0].weight.grad.data[0].numpy(), model[0].bias.grad.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1ba432c4e95c429c6e3c8f0d3cdb094",
     "grade": false,
     "grade_id": "cell-be4fc31c6e17ef18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "**Note:** Even though the objects and shapes are different, do the output results match with the previous results ([NumPy](#1.B.a.-Numpy-implementation-(4-points)) and [PyTorch](#1.B.b.-PyTorch-implementation-(2-points)) implementations)?\n",
    "</div> \n",
    "\n",
    "# 2. Binary Classification\n",
    "[Back to index](#Index)\n",
    "\n",
    "It is a common wisdom in deep learning that one needs to go deeper (more layers) to learn a complex function such as an image classifier. However, managing the computational graph becomes very difficult as the network architecture becomes more complicated and the number of parameters increases. Thus, implementing such a network with Numpy is neither feasible (don't worry, we won't make you code a multilayer binary classifier in NumPy) nor efficient. In this section, we will implement a whole pipeline for training a binary classifier using PyTorch. \n",
    "\n",
    "## Data preparation and visualization\n",
    "### Creating a two moon dataset\n",
    "\n",
    "We will take advantage of this section to introduce the library [Scikit Learn](https://scikit-learn.org/stable/). Just as Scikit Image, it is one of the [Scipy Kits](https://www.scipy.org/scikits.html) that builds on top of the core modules of SciPy for a specific application, in this case, machine learning. Scikit Learn provides everything from implementation of machine learning algorithms, to standard toy datasets and functions for common data preprocessing. \n",
    "\n",
    "For this toy example, we will train a network that takes two-dimensional coordinates and classifies them into two classes. To this end, we will use a two moon dataset that we will generate with the function [`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) in the [`sklearn.datasets`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) package and split the samples into a training and a test dataset (we are sure you have heard about this before, but here is a crash course [here](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/video-lecture) on the concept) using `train_test_split` in the `sklearn.model_selection` package (please refer to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for a complete reference). This function splits the data randomly into two sets according to the ratio that we choose. One set will be used during *training*, and a second that will be used to *test* the quality of the resulting model in data it has not been trained on.\n",
    "\n",
    "Run the next cell to create and split the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d217af42c778d761a2a1b7a4c53735b",
     "grade": false,
     "grade_id": "cell-336889e978aaf2ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "nb_samples = 1000\n",
    "# ratio between training and test sets\n",
    "test_size = 0.1\n",
    "\n",
    "# Crete data set\n",
    "X, y = make_moons(n_samples = nb_samples, noise=0.2, random_state=100)\n",
    "# Add dimension to y for later use \n",
    "y = y.reshape((-1,1))\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "print('X train shape: ', X_train.shape, '\\nX test shape: ', X_test.shape)\n",
    "# Take a look at a point\n",
    "print(f'\\nSample point:\\n(x1,x2) coordinates: {X[0]}, y label: {y[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9210585541d392935edda98635521117",
     "grade": false,
     "grade_id": "cell-e6e307fb9e3c6850",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, our data consists of coordinate pairs (corresponding to `X`) that belong to a class (specified by `y`).\n",
    "\n",
    "Run the next cells and check how the data looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c46c460bf87a3998403527aa3022a9c",
     "grade": false,
     "grade_id": "cell-c0f897f8988515c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# Plot class zero\n",
    "idx_0 = np.where(y == 0)\n",
    "plt.scatter(X[idx_0, 0], X[idx_0, 1], s = 6, color = 'c', label = 'Class 0')\n",
    "# Plot class one\n",
    "idx_1 = np.where(y == 1)\n",
    "plt.scatter(X[idx_1, 0], X[idx_1, 1], s = 6, color = 'm', label = 'Class 1')\n",
    "# Set labels, title, grid and legend\n",
    "plt.ylabel('y'); plt.xlabel('x'); plt.title('2 Moons Dataset'); plt.grid(); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c8c5fb2464c48bb71b91a949b5344a6",
     "grade": false,
     "grade_id": "cell-c74e754a84ffb844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, this data set is far more interesting than the ones we have been working with before. The boundary is evidently not linear, and the two classes are not perfectly separable with a smooth curve, as there is some overlap between the sets. Nonetheless, there is a clear boundary and it is still only 2D, so deep learning should be able to handle it. On to the fun part.\n",
    "\n",
    "## 2.A. PyTorch Implementation (9 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "In this section, you will code a small (yet more complex than the ones we have been working on) neural network with PyTorch. This section will allow you to interact by yourself with the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) module and all its functionalities, and get used to the process of developing a neural network from scratch with PyTorch. \n",
    "\n",
    "First, run the next cell to convert our data points to tensors. Pay attention to all the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "668ac7899034bb2f4fbc77eb3fdeda94",
     "grade": false,
     "grade_id": "cell-795b43c0cf53083c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the values of input variables\n",
    "X_train_tc = torch.tensor(X_train, requires_grad = False, dtype=torch.float64)\n",
    "X_test_tc = torch.tensor(X_test, requires_grad = False, dtype=torch.float64)\n",
    "y_train_tc = torch.tensor(y_train, requires_grad = False, dtype=torch.float64)\n",
    "y_test_tc = torch.tensor(y_test, requires_grad = False, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10a17fb113d17b27b7f24bc50620b5a8",
     "grade": false,
     "grade_id": "cell-fa7232f0dc999505",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you need to define a model. Use the `torch.nn.Sequential` module to build the model with the following specifications:\n",
    "\n",
    "```python\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Specifically, this model starts with a linear layer of `in_features = 2` and `out_features = 25` followed by the activation function [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), which is [the most popular activation function](https://towardsdatascience.com/understanding-relu-the-most-popular-activation-function-in-5-minutes-459e3a2124f) for building modern deep neural networks: \n",
    "\n",
    "$$ReLU(x)=(x)_+ =\\max\\,(0,x)$$\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/ReLU.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "The model finishes with the sigmoid function so that it outputs the value between 0 and 1, which can be interpreted as a probability. \n",
    "You will need to use `torch.nn.Linear`, `torch.nn.ReLU()`, and `torch.nn.Sigmoid()` modules in order.\n",
    "\n",
    "For **2 points**, code the function `build_model`, that takes no parameters and returns a `model` with the specified characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b3f31afd6e7f74c21eaf64809221641",
     "grade": false,
     "grade_id": "cell-cd56507225bb4ebe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Declare function build model\n",
    "def build_model():\n",
    "    model = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Cast model parameters to float64\n",
    "    model.double()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdd0314c2178b285bdd2755fe30068c8",
     "grade": false,
     "grade_id": "cell-afcb9e237f70bc54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's run your function and print the resulting model. Look back at the architechture we proposed and check that it is correct. Then we will make some tests on your architecture, to make sure that it is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0bbf1745b91b20327699cf175f57015",
     "grade": true,
     "grade_id": "cell-2b5b7025d07b92ec",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "print(model)\n",
    "\n",
    "# Check that the model has 5 layers\n",
    "assert len(model._modules.items()) == 8, f'The model should have 8 layers: 4 linear, 3 ReLU and 1 Sigmoid. Yours currently has {len(model._modules.items())} layers.'\n",
    "# Check that the layer types are correct\n",
    "for i, layer in enumerate(model._modules.items()):\n",
    "    current_layer = torch.nn.modules.linear.Linear if i % 2 == 0 else torch.nn.modules.activation.ReLU\n",
    "    if i == 7: current_layer = torch.nn.modules.activation.Sigmoid \n",
    "    assert type(layer[1]) == current_layer, f'Layer {i} should be of type {current_layer}, not {type(layer[1])}.'\n",
    "# Check that the input and output size of the layers is correct and that torch.nn.Sequential was used\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if i == 0:\n",
    "        assert type(module) == torch.nn.modules.container.Sequential, 'You should use torch.nn.Sequential to build your model!'\n",
    "    if i == 1:\n",
    "        assert module.in_features == 2, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that we are trying to classify a 2D dataset.'\n",
    "        assert module.out_features == 25, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the first hidden layer should be of size 25.'\n",
    "    if i == 3:\n",
    "        assert module.in_features == 25, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the first hidden layer should be of size 25.'\n",
    "        assert module.out_features == 50, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the second hidden layer should be of size 50.'\n",
    "    if i == 5:\n",
    "        assert module.in_features == 50, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the second hidden layer should be of size 50.'\n",
    "        assert module.out_features == 25, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the third hidden layer should be of size 25.'\n",
    "    if i == 7:\n",
    "        assert module.in_features == 25, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the third hidden layer should be of size 25.'\n",
    "        assert module.out_features == 1, f'The output size of layer {i-1} ({module.out_features}) is not correct. We will classify with a scalar between 0 and 1, so we need one single output.'\n",
    "print('Well done! The model seems to be correct.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d2f43be34306bf0b24155e885fe575b",
     "grade": false,
     "grade_id": "cell-1814b31d116d5f3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "PyTorch provides a list of loss functions that are frequently used for training neural networks: [torch.nn loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions). Since we are solving the binary classification problem, we need a binary cross entropy (BCE) loss (full documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)), which you will explore in more detail in the second notebook of this lab. \n",
    "\n",
    "To optimize the model parameters, we can use [torch.optim](https://pytorch.org/docs/stable/optim.html). Here, we will use [torch.optim.SGD](https://pytorch.org/docs/master/generated/torch.optim.SGD.html) and [torch.optim.Adam](https://pytorch.org/docs/master/generated/torch.optim.Adam.html) for training our toy network. \n",
    "\n",
    "In the next cell, **for 2 points**, code the functions `define_loss` and `define_optimizers`, **1 point each**.\n",
    "\n",
    "* `define_loss` has no input parameters, and should return the `torch` module `BCELoss`,\n",
    "* `define_optimizer` has as input parameters `model` (a PyTorch model) and `learning_rate` (scalar, self-explanatory) , and should return the `torch` module `SGD`,\n",
    "\n",
    "<div class = 'alert alert-success'>\n",
    "<b>Hint:</b> As we showed in the previous exercises, one needs to specify the model parameters that are to be trained. In PyTorch, one can call the parameters of a Module by <code>.parameters()</code>. For example, since we want to train the entire trainable parameters that are included our <code>model</code> (Sequential Module), we can use <code>model.parameters()</code>. \n",
    "</div>\n",
    "\n",
    "Complete the two functions and then run the following cells for some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afbde4587a7271fbd31547f4bcffde67",
     "grade": false,
     "grade_id": "cell-a121d09571c49084",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Declare loss function\n",
    "def define_loss():\n",
    "    bce_loss = None\n",
    "    \n",
    "    # set the loss function loss_fn\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return bce_loss\n",
    "\n",
    "def define_optimizer(model, learning_rate):\n",
    "    optimizer = None  \n",
    "    \n",
    "    # set the SGD optimizer with the given learning rate for the model model \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c814f9e9253c1d3e76f7028cfcd0fb6a",
     "grade": false,
     "grade_id": "cell-b9ccd63303bdb4f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "<b>Hint</b>: Apart from the instantiation of the variable and the return value, both of your functions above should be one-liners. \n",
    "</div>\n",
    "\n",
    "Run the next two cells to examine your loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96ecc17eae51b714d4519fe0603c3ba",
     "grade": true,
     "grade_id": "cell-b952ca6156805a1a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Examine loss. First, we declare it\n",
    "bce_loss = define_loss()\n",
    "if not bce_loss.__module__ == 'torch.nn.modules.loss': print('Your loss does not seem to be a PyTorch loss module!')\n",
    "elif not 'BCE' in str(bce_loss):  print('Your loss is a PyTorch loss module, just not the correct one.')\n",
    "else: print('Great! You\\'re on the right track for building your network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48fec90af99820632a19b33719bf2f0e",
     "grade": true,
     "grade_id": "cell-2c8349c78efcb3c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Examine loss. First, we declare it\n",
    "optimizer = define_optimizer(model, 0.01)\n",
    "\n",
    "if 'torch.optim' not in optimizer.__module__: print('Your optimizer does not seem to be a PyTorch optimizer module!')\n",
    "elif not 'sgd' in optimizer.__module__:  print('Your optimizer is a PyTorch optimizer, just not the correct one.')\n",
    "else: print('You\\'re really close from finishing your network!.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d183f611e31c44e0c5d3bd6168b7221f",
     "grade": false,
     "grade_id": "cell-16074b36407a5704",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we have all the ingredients to train the network. Make sure that you pass the previous sanity checks, as the performance of your network depends on it!\n",
    "\n",
    "In the next cell, for **5 points** complete the function `train`. It takes as input parameters:\n",
    "\n",
    "* `X_train_tc`: (of shape `(n, 2)`): the data points.\n",
    "* `y_train_tc`: (of shape `(n, 1)`): the true class of each of the data points.\n",
    "* `model`: a PyTorch model, e.g. the output of your function `build_model`.\n",
    "* `optimizer`: a PyTorch optimizer, e.g. the output of your function `define_optimizer`.\n",
    "* `loss_fn`: a PyTorch loss function, e.g. the output of your function `define_loss`.\n",
    "* `epochs`: Number of times that the optimizer will go through the whole data set.\n",
    "\n",
    "It returns the variables `y_pred` and `loss`, the predictions and loss of the last epoch (on the training set). Note that there is no need to return the model, as it should directly modify the parameters of the model given as parameter.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "<b>Hints</b>\n",
    "In the function <code>train</code>, you will have to perform the following tasks (in order):\n",
    "<ol><li>Perform a forward pass,</li>\n",
    "<li>Calculate the loss between the predicted and the true values</li>\n",
    "<li>Perform the backward pass</li>\n",
    "<li>Update the parameters of your model</li>\n",
    "<li>Reset the gradients of the parameters to $0$</li></ol>\n",
    "    \n",
    "Note that all of this tasks are one-liners. In 2 of them, you need to perform an assignment. On the other three, you need to call methods of the parameters of the function `train`. Only the highest level of abstraction of PyTorch is accepted! (Anyway, we don't think you will want to perform the backward pass manually).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5faa46b53d83515bc9cfe651af395b7",
     "grade": false,
     "grade_id": "cell-e7f2565431f41844",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(X_train_tc, y_train_tc, model, optimizer, loss_fn, epoch, verbose=True, save_preds=None):\n",
    "    # Prepare grid to evaluate the decision boundary for later visualization\n",
    "    if save_preds != None:\n",
    "        xv, yv = np.meshgrid(np.linspace(-1.5, 2.5, 201), np.linspace(-1, 1.5, 126))\n",
    "        X_pred = np.concatenate([xv.reshape(-1, 1), yv.reshape(-1, 1)], axis=1)\n",
    "    # Print divisor\n",
    "    div = 1 if epoch < 10 else epoch//10\n",
    "    \n",
    "    for iter_num in range(epoch):\n",
    "        \n",
    "        # perform the forward pass\n",
    "        y_pred = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # compute the loss in the variable loss\n",
    "        loss = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # perform the backward pass\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # update the parameters\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # clear up the accumulated gradients\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        model.eval()\n",
    "        # print loss\n",
    "        if iter_num % div == 0:    \n",
    "            if verbose:\n",
    "                y_pred_test = model(X_test_tc)\n",
    "                pred = y_pred_test >= 0.5\n",
    "                truth = y_test_tc >= 0.5\n",
    "                acc = pred.eq(truth).sum().item() / truth.numel()\n",
    "                print(f'iteration {iter_num}, loss: {loss:4f}, test accuracy: {acc:4f}')\n",
    "        if iter_num % 2 == 0:\n",
    "            if save_preds != None:                \n",
    "                prediction_probs = model(torch.tensor(X_pred, dtype=torch.float64))\n",
    "                save_preds.append(prediction_probs.data.numpy().reshape(xv.shape))\n",
    "        model.train()\n",
    "    return y_pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "413898b29f0bd96f3205943879aef20a",
     "grade": false,
     "grade_id": "cell-1e1f497e0304dff1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "\n",
    "**Note:** You might have noticed that there are some new attributes `eval()` and `train()` that we used inside the if statement for printing the results. `model.eval()` tells the model that you are now in the testing mode, and `model.train()` simply tells it that you are in the training mode. This becomes important for layers like `dropout`, `batchnorm`, which behave differently for the train and test procedures. In fact, we do not need these for our toy model since it does not have such layers. However, we included here to show you that it is always recommended to specify whether your model is under the training mode or testing mode in practice. \n",
    "</div> \n",
    "\n",
    "We hope that it didn't take too long. Run the next cell to see whether everything is correct by running your function for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b02110cb6555807ba623f379ceff9a3",
     "grade": true,
     "grade_id": "cell-cb54e07263ad2e2d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_, _ = train(X_train_tc, y_train_tc, model, optimizer, bce_loss, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fcb7c2ad572860aa0d7c351e62b29719",
     "grade": false,
     "grade_id": "cell-7b9a6978fa72c3ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Did you see anything interesting? No? Let's just try again, why not. Run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0ddbf586288af3de13009def8ba0410",
     "grade": true,
     "grade_id": "cell-cb3f404f92b4ede6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_, _ = train(X_train_tc, y_train_tc, model, optimizer, loss_fn, epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6bdbe35331cd2df877871fd16c818c4",
     "grade": false,
     "grade_id": "cell-41ba67dd13699a93",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sorry, we are not going to make you run the same cell pointlessly. In fact, with the learning rate we have chosen, SGD can take quite a long time. Just for the purpose of seeing that it actually works, run the next cell, where we will run it for a few more epochs than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4faa55726ddb37109badc18842e5c6a7",
     "grade": true,
     "grade_id": "cell-f5972cf82f8b0d64",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_, _ = train(X_train_tc, y_train_tc, model, optimizer, loss_fn, epoch=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d69fc2c36cf785783b40776227534293",
     "grade": false,
     "grade_id": "cell-e513cd0cc0817316",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, SGD does work, the loss will decreases but very slowly. If we had a huge dataset of thousands of dimensions (basically any image processing problem), we would appreciate something faster. Now, let's try a different optimizer, `torch.optim.Adam` with default settings. Adam optimizer uses the notion of momentum, which is known to accelerate the training process. For more detailed explanation, please check [torch.optim.Adam](https://pytorch.org/docs/master/generated/torch.optim.Adam.html). \n",
    "\n",
    "Run the next cell to reset the parameters of your model, and declare a new optimizer in the variable `optimizer_adam` (no need to declare a function or anything, just define the optimizer). This will count for **1 point**, in combination with its proper use in the function `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3edfdeb67e57786f85c2be0711a33180",
     "grade": false,
     "grade_id": "cell-02b035fd90ff9862",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's first reset the parameters for a fair comparison.\n",
    "for layer in model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "        \n",
    "# Set the Adam optimizer with the given learning rate (learning_rate)\n",
    "optimizer_adam = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eac4d568edec609a8969d55ac15c5b11",
     "grade": false,
     "grade_id": "cell-8652b6341da808b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Just for a very quick test, run the next cell to explore your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0baad693ded273fc9aaef657bfcb5258",
     "grade": true,
     "grade_id": "cell-de3b258838769c3b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Look at your optimizer\n",
    "print(optimizer_adam)\n",
    "# Check that it is the correct one\n",
    "if not 'Adam' in str(optimizer_adam): \n",
    "    print('Your optimizer does not seem to be the correct one!')\n",
    "else: \n",
    "    print('Well done! You now know 2 of the most important optimizers in deep learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc910a4df27285edea59e396ef106805",
     "grade": false,
     "grade_id": "cell-1d24f80a61fd963c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell and check the effect of using different optimizers. Note how we are only training for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fb2bd49ae00ac19ae8bca1351bc508a",
     "grade": true,
     "grade_id": "cell-1bb106b6c41b1f5c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decision_boundary_preds = []\n",
    "# Run with Adam optimizer \n",
    "_, _ = train(X_train_tc, y_train_tc, model, optimizer_adam, loss_fn, epoch=50, save_preds=decision_boundary_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ef987e0204e245101f17c07760c6768",
     "grade": false,
     "grade_id": "cell-4c0b904ed3f7abd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "<b>PyTorch appreciation:</b> Imagine how difficult it would have been if you had to implement all these in Numpy while keeping the framework flexible to different architectures (e.g., layers, feature dimensions, etc.) and taking care of all the forward and backward passes in a concise manner. \n",
    "<!-- There are two goals in this exercise. One is to go over the entire learning process with PyTorch, and the other is to demonstrate how much easier it can be to implement the network using PyTorch than using a pure Numpy.  \n",
    "     -->\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4db04b73989873bf6029e1b086ce5c0d",
     "grade": false,
     "grade_id": "cell-abaf4333342ea4ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.B. Visualization\n",
    "[Back to index](#Index)\n",
    "\n",
    "For the final section of the lab, we want you to take a look at the evolution of the decision boundary. Run the next cell, and play with the gif to see this evolution throughout the 50 epochs of training with Adam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8b511df07e91c8c25186b1172b637bd",
     "grade": false,
     "grade_id": "cell-62ec06da3fb2aa0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Callback function for the play widget\n",
    "def play_func(value):\n",
    "    i = int(value['new'] // 2)\n",
    "    ax = plt.gca()\n",
    "    ax.clear()\n",
    "    # Display decision boundary\n",
    "    ax.imshow(decision_boundary_preds[i], origin='lower', extent=(-1.5, 2.5, -1, 1.5))\n",
    "    # Plot the dataset points\n",
    "    idx_0 = np.where(y == 0)\n",
    "    ax.scatter(X[idx_0, 0], X[idx_0, 1], s = 6, color = 'c', label = 'Class 0')\n",
    "    idx_1 = np.where(y == 1)\n",
    "    ax.scatter(X[idx_1, 0], X[idx_1, 1], s = 6, color = 'm', label = 'Class 1')\n",
    "    ax.set_xlim([-1.5, 2.5])\n",
    "    ax.set_ylim([-1, 1.5])\n",
    "    ax.set_title(f'Decision boundary at iteration {2*i}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.close('all')\n",
    "# Initialize the figure\n",
    "fig, ax = plt.subplots(1)\n",
    "play_func({'new':0})\n",
    "plt.show()\n",
    "# Create the play widget\n",
    "play = widgets.Play(value=0, min=0, max=(len(decision_boundary_preds)-1)*2, step=2, interval=1000, description=\"Press play\")\n",
    "# Add callback function\n",
    "play.observe(play_func, names='value')\n",
    "# Add slider\n",
    "slider = widgets.IntSlider(min=0, max=(len(decision_boundary_preds)-1)*2, step=2)\n",
    "widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "# Display\n",
    "widgets.HBox([play, slider])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "530b0c89fe7bb54520ca342a0521c60b",
     "grade": false,
     "grade_id": "cell-7de6189b1408068b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "<h4>Explore!</h4>\n",
    "Try with different setups such as different number of layers, feature dimensions, activation functions, etc., and see how it goes. \n",
    "    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<p><b>Congratulations on finishing the first part of the Neural Networks lab!</b></p>\n",
    "<p>\n",
    "Make sure to save your notebook (you might want to keep a copy on your personal computer) and upload it to <a href=\"https://moodle.epfl.ch/mod/assign/view.php?id=1157357\">Moodle</a>, in a zip file with other notebooks of this lab.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "* Keep the name of the notebook as: *1_NN_Basics.ipynb*,\n",
    "* Name the zip file: *Neural_Networks_Lab.zip*.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<h4>Feedback</h4>\n",
    "    <p style=\"margin:4px;\">\n",
    "    This is the first edition of the image-processing laboratories using Jupyter Notebooks running on Noto. Do not leave before giving us your <a href=\"https://moodle.epfl.ch/mod/feedback/view.php?id=1157363\">feedback here!</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
