{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2934477c7af628ff464554801f459dba",
     "grade": false,
     "grade_id": "cell-99f58b9017e8efe1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Image Processing Laboratory Notebooks</h2>\n",
    "<hr style=\"clear:both\">\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "This Juypter notebook is part of a series of computer laboratories which are designed\n",
    "to teach image-processing programming; they are running on the EPFL's Noto server. They are the practical complement of the theoretical lectures of the EPFL's Master course <b>Image Processing II</b> \n",
    "(<a href=\"https://moodle.epfl.ch/course/view.php?id=463\">MICRO-512</a>) taught by Dr. D. Sage, Dr. M. Liebling, Prof. M. Unser and Prof. D. Van de Ville.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:2px; text-align:justify\">\n",
    "The project is funded by the Center for Digital Education and the School of Engineering. It is owned by the <a href=\"http://bigwww.epfl.ch/\">Biomedical Imaging Group</a>. \n",
    "The distribution or the reproduction of the notebook is strictly prohibited without the written consent of the authors.  &copy; EPFL 2021.\n",
    "</p>\n",
    "<p style=\"font-size:0.85em; margin:0px\"><b>Authors</b>: \n",
    "    <a href=\"mailto:pol.delaguilapla@epfl.ch\">Pol del Aguila Pla</a>, \n",
    "    <a href=\"mailto:kay.lachler@epfl.ch\">Kay Lächler</a>,\n",
    "    <a href=\"mailto:alejandro.nogueronaramburu@epfl.ch\">Alejandro Noguerón Arámburu</a>,\n",
    "    <a href=\"mailto:daniel.sage@epfl.ch\">Daniel Sage</a>,\n",
    "    <a href=\"mailto:jaejun.yoo@epfl.ch\">Jaejun Yoo</a>, and\n",
    "    <a href=\"mailto:kamil.seghrouchni@epfl.ch\">Kamil Seghrouchni</a>.\n",
    "     \n",
    "</p>\n",
    "<hr style=\"clear:both\">\n",
    "<h1>Lab 7.2: Neural Networks: Application</h1>\n",
    "<div style=\"background-color:#F0F0F0;padding:4px\">\n",
    "    <p style=\"margin:4px;\"><b>Released</b>: Thursday May 27, 2021</p>\n",
    "    <p style=\"margin:4px;\"><b>Submission</b>: <span style=\"color:red\">Friday June 11, 2021</span> (before 11:59PM) on <a href=\"https://moodle.epfl.ch/course/view.php?id=463\">Moodle</a></p>\n",
    "    <p style=\"margin:4px;\"><b>Grade weigth</b>: Lab 7 (28 points), 7.5 % of the overall grade</p>\n",
    "    <p style=\"margin:4px;\"><b>Remote help</b>: Thursday 3 June and Monday 7 June, 2021 on Zoom (see Moodle for link)</p>    \n",
    "    <p style=\"margin:4px;\"><b>Related lectures</b>: Chapter 11</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: \n",
    "### SCIPER: \n",
    "\n",
    "Double-click on this cell and fill your name and SCIPER number. Then, run the cell below to verify your identity in Noto and set the seed for random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e24ab6f2099ca85a90d1dda4e563a89",
     "grade": true,
     "grade_id": "cell-aa5326c6d1773ea8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "# This line recovers your camipro number to mark the images with your ID\n",
    "uid = int(getpass.getuser().split('-')[2]) if len(getpass.getuser().split('-')) > 2 else ord(getpass.getuser()[0])\n",
    "print(f'SCIPER: {uid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b05edc355dd5540d26798f5f1c46b6fc",
     "grade": false,
     "grade_id": "cell-a6f05f88b2393ac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <a name=\"imports_\"></a> Imports\n",
    "In the next cell we import Python libraries we will use throughout the lab.\n",
    "<!-- , as well as the `IPLabViewer` class, created specifically for this course, which provides interactive image visualization based on the `ipywidgets` library: -->\n",
    "* [`matplotlib.pyplot`](https://matplotlib.org/3.2.2/api/_as_gen/matplotlib.pyplot.html), to display images,\n",
    "<!-- * [`ipywidgets`](https://ipywidgets.readthedocs.io/en/latest/), to make the image display interactive, -->\n",
    "* [`numpy`](https://numpy.org/doc/stable/reference/index.html), for mathematical operations on arrays,\n",
    "* [`torch`](https://pytorch.org/), for comparing the results of our manual implementation with pytorch autograd,\n",
    "* [`sklearn`](https://scikit-learn.org/stable/), for the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "<!-- We will then load the `IPLabViewer` class (see the documentation [here](https://github.com/Biomedical-Imaging-Group/IPLabImageViewer/wiki/Python-IPLabViewer()-Class) or run the Python command `help(viewer)` after loading the class). -->\n",
    "\n",
    "Finally, we load the images you will use in the exercise to test your functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95ca54988850123014ff52e5e329fc69",
     "grade": false,
     "grade_id": "cell-e3e2552bf2b6a196",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2 as cv \n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import IPLabViewer() Class\n",
    "import sys\n",
    "sys.path.insert(0, 'lib')\n",
    "from iplabs import IPLabViewer as viewer\n",
    "\n",
    "%matplotlib widget\n",
    "# Fix random seeds for reproducible results\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Load the images\n",
    "input_img = cv.imread('images/input.png', cv.IMREAD_UNCHANGED)\n",
    "label_img = cv.imread('images/label.png', cv.IMREAD_UNCHANGED)\n",
    "input_img_test = cv.imread('images/test_input.tiff', cv.IMREAD_UNCHANGED)\n",
    "label_img_test = cv.imread('images/test_label.tiff', cv.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a01414a131de298b336acaf44f08c255",
     "grade": false,
     "grade_id": "cell-2e47d9f9ce3f6ea1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Neural Networks: Application (8 points)\n",
    "\n",
    "In this laboratory we propose to study some applications of neural networks, namely pixel classification. If you still have questions concerning either programming or the lab in general, don't hesitate to contact one of the TAs listed on [Moodle](https://moodle.epfl.ch/course/view.php?id=463).\n",
    "\n",
    "## Index    \n",
    "1. [Multiclass pixel classification](#1.-Multiclass-pixel-classification)\n",
    "    1. [Data preparation](#1.A.-Data-preparation)\n",
    "        1. [Creating the input and target dataset](#1.A.a.-Creating-the-input-and-target-dataset)\n",
    "        2. [Split the datasets into train and validation sets](#1.A.b.-Split-the-datasets-into-train-and-validation-sets)\n",
    "        3. [Creating the DataLoader](#1.A.c.-Creating-the-DataLoader)\n",
    "    2. [Softmax and cross-entropy loss for multiclass classification](#1.B.-Softmax-and-cross-entropy-loss-for-multiclass-classification)\n",
    "        1. [Softmax in NumPy](#1.B.a.-Softmax-in-NumPy-(1-point)) **(1 point)**\n",
    "        2. [Softmax in PyTorch](#1.B.b.-Softmax-in-PyTorch)\n",
    "        3. [Cross-entropy in NumPy](#1.B.c.-Cross-entropy-in-NumPy-(2-points)) **(2 points)**\n",
    "        4. [Cross-entropy in PyTorch](#1.B.d.-Cross-entropy-in-PyTorch)\n",
    "    3. [Build a model and train it with PyTorch](#1.C.-Build-a-model-and-train-it-with-PyTorch)\n",
    "        1. [Define the model](#1.C.a.-Define-the-model-(2-points)) **(2 points)**\n",
    "        2. [Complete the training pipeline](#1.C.b.-Complete-the-training-pipeline-(2-points)) **(2 points)**\n",
    "        3. [Test the model on a test image](#1.C.c-Test-the-model-on-a-test-image)\n",
    "        4. [Classwise accuracy](#1.C.d.-Classwise-accuracy)\n",
    "    4. [Class imbalance problem](#1.D.-Class-imbalance-problem)\n",
    "        1. [Add weights to the loss function](#1.D.a.-Add-weights-to-the-loss-function-(1-point)) **(1 point)**\n",
    "\n",
    "\n",
    "# 1. Multiclass pixel classification\n",
    "[Back to index](#Index)\n",
    "\n",
    "Throughout this lab, we will focus on applying neural networks to the characterization of the cell nuclei of histopathology tissue. This is a routine task in clinics, and it is generally done on color stained images. The most widely used stain is H&E (Hematoxylin and Eosin), that highlights the nuclei in dark color. \n",
    "\n",
    "Run the cell below to visualize the images that we will use for this.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> If you click on <code>Options</code>, you can enable the <code>Joint Zoom</code> and zoom into different areas of the image to see how the pixels are classified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8788a759a28c90eb779198d6aec0a8f",
     "grade": false,
     "grade_id": "cell-86ac774105d1677b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "view = viewer([input_img, label_img], title=['Input Image', 'Grount Truth Classification'], subplots=(1,2), widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b2738f60a7007df4e298ec9ebfd36e8",
     "grade": false,
     "grade_id": "cell-201b9662c77b9855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see in the images above, in the label we have three color levels, $1)$ nuclei in brown, $2)$ nuclei in dark purple, and $3)$ background in bright purple. The goal is to automatically segment the image by assigning one of these three classes to every pixel of the input image. For this lab, using the above images as training dataset, we will train a fully connected neural network to classify pixels based on their three input values: red, green and blue (RGB). Then, we will test this trained network on a larger stained image.\n",
    "\n",
    "## 1.A. Data preparation\n",
    "### 1.A.a. Creating the input and target dataset\n",
    "[Back to index](#Index)\n",
    "\n",
    "First, we need to create our desired output classes. As we know, the label image consists of only three distinct RGB values, so we assign a number $(0$, $1$ or $2)$ to each one of these values and create an array that maps each pixel of the labeled image to one of these three classes. For this we use the [`np.unique`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function with the argument `return_inverse=True`, which returns the three unique RGB values as well as the mentioned array that maps each pixel to one of the classes.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> Since the input to our neural network will be a vector of RGB values, we flatten the labeled image into a vector as well, before assigning the pixels to their classes. For that, we use the <code>np.array</code> method <code>reshape</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce488d74e06c8e27a2ab3094fd9bd489",
     "grade": false,
     "grade_id": "cell-764f8c7853a1455a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create labels\n",
    "pixel_rgb_val, train_label_class_index = np.unique(label_img.reshape((-1, 3)), axis=0, return_inverse=True)\n",
    "print(f'The three unique RGB values of the labeled image are:\\n{pixel_rgb_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19ff2c66b401a196525558b8d78361b7",
     "grade": false,
     "grade_id": "cell-8170c5f2a0148d89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we create the PyTorch tensors `net_input` and `net_target` from our images. The input should be a vector of RGB values and the output should be a simple vector with the corresponding classes. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "271f9fbc6c79f09880158098414d3f27",
     "grade": false,
     "grade_id": "cell-42dbb4d02ed5b11a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The input should be of type float\n",
    "net_input = torch.FloatTensor(input_img.reshape((-1, 3)))\n",
    "# The target class should be of type long\n",
    "net_target = torch.LongTensor(train_label_class_index)\n",
    "print(f'Input size: {net_input.shape}, target size: {net_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fa823b12c0d39bcf7990f9498a2d553",
     "grade": false,
     "grade_id": "cell-87141478a4ebe508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.b. Split the datasets into train and validation sets\n",
    "[Back to index](#Index)\n",
    "\n",
    "The size of our cell image is $(150 \\times 150)$ pixels and thus, we have a total of $150 \\times 150 = 22\\,500$ RGB pixels. In order to monitor the training process, we need to split our dataset into a training and a validation set, which is typically done in a ratio of $20%\\$ validation to $80\\%$ training. The difference between a test set, as we used it in the first notebook, and a validation set, is that the validation set helps us to monitor the performance of the model during the training process, while a test set is usually used to measure the performance of the final trained model. As such, a validation set is usually used to calculate the accuracy of the model on new data at each training step, which helps to detect [overfitting](https://www.investopedia.com/terms/o/overfitting.asp). For this we will again use the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from [`sklearn`](https://scikit-learn.org/stable/) as in the previous notebook, but here the generated test set will be our validation set. Run the cell below to create the train and validation sets.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> As mentioned before, our <b>test set</b> will consist of one larger stained cell image that we loaded as <code>input_img_test</code> and <code>label_img_test</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86bf4e353ad6a1ae4cc2175c0a9bd03f",
     "grade": false,
     "grade_id": "cell-3ef3a300ee56efc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ratio between training and validation sets\n",
    "validation_ratio = 0.2\n",
    "\n",
    "splitting = train_test_split(net_input, net_target, test_size=validation_ratio, random_state=0)\n",
    "net_train_input, net_valid_input, net_train_target, net_valid_target = splitting\n",
    "print(f'The training set contains {len(net_train_input)} pixels and the validation set contains {len(net_valid_input)} pixels.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab0059cda7a8b252f6728b039b7ab923",
     "grade": false,
     "grade_id": "cell-6c4c4409d41f5ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.A.c. Creating the DataLoader\n",
    "[Back to index](#Index)\n",
    "\n",
    "In the [first part of the lab](./1_NN_Basics.ipynb), we used very few samples to train our networks and so we could simply use the entire dataset at once for the training. Now that we have a lot more samples, we need to train our model on small batches, called minibatches, of the entire dataset, otherwise there might not be enough memory available to handle the calculation of all the derivatives. \n",
    "\n",
    "To do this, we will create a `DataLoader` that serves 100 samples as a minibatch for each training step. \n",
    "\n",
    "PyTorch provides two data primitives: [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). They allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable* around the Dataset to enable easy access to the samples. \n",
    "\n",
    "In this exercise, we will see how to make a custom `Dataset` class. \n",
    "`torch.utils.data.Dataset` is an abstract class representing a dataset and its template is as follows:\n",
    "```python\n",
    "class Your_Custom_Dataset_Name(Dataset):\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "```\n",
    "\n",
    "Your custom dataset should inherit `Dataset` and override the following methods:\n",
    "\n",
    "* **`__len__`** so that `len(dataset)` returns the size of the dataset.\n",
    "* **`__getitem__`** to support the indexing such that `dataset[i]` can be used to get the $i^{\\mathrm{th}}$ sample of the dataset.\n",
    "\n",
    "If you want to see another example, you can check out the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) on the Dataset class. \n",
    "\n",
    "_*\"An iterable is any Python object capable of returning its members one at a time, permitting it to be iterated over in a for-loop.\"_\n",
    "\n",
    "Run the cell below to define the custom `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c44f831c5cd3b468b8b7574fae173d63",
     "grade": false,
     "grade_id": "cell-a18f70de364ec2fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Use the dataset template for your own data! \n",
    "class CellDataset(Dataset):\n",
    "    # Initializes the dataset with our input data and labels\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    # Returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Returns the item at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        # We return the input and target values\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Train in batch mode\n",
    "batch_size = 100\n",
    "    \n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset=CellDataset(net_train_input, net_train_target), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=CellDataset(net_valid_input, net_valid_target), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'train_loader:\\n\\tnumber of batches: {len(train_loader)}, samples per batch: {len(iter(train_loader).__next__()[0])}')\n",
    "print(f'valid_loader:\\n\\tnumber of batches: {len(valid_loader)}, samples per batch: {len(iter(valid_loader).__next__()[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5af12aecc3a47d200bdeb391ba923097",
     "grade": false,
     "grade_id": "cell-56d690a3154f8b5a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.B. Softmax and cross-entropy loss for multiclass classification\n",
    "[Back to index](#Index)\n",
    "\n",
    "To solve a classification problem using a neural network, at some point, one needs to map a network output $x$ (which is unbounded) into the value between $0$ and $1$ so that it can be interpreted as a probability for each class $i\\in \\{1, \\ldots, C\\}$. For the binary classification problem in the previous notebook, we used the sigmoid function to map the single output value to the range $[0,1]$. Now, for the multiclass classification problem, we need a function that can act on multiple values at once, for which, we will use the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function:\n",
    "\n",
    "$$\\sigma(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^Ce^{x_j}}$$\n",
    "where $C$ is the total number of classes (in our case 3).\n",
    "\n",
    "In the deep learning community, $x$ is typically referred to as `logits`, which is the vector of raw predictions that a classification model generates. For the multiclass classification problem, $x$ is normalized by using the softmax function, which generates a vector of probabilities corresponding to each of the classes, and $\\Sigma_{i=0}^{N} \\sigma(x)_i  = 1$, that is, the sum of all the elements of this probability vector is $1$. Then, we can measure the performance of a classification model using the `cross_entropy` loss,  which increases as the predicted probability diverges from the actual label. \n",
    "\n",
    "In the next sections we will go through a series of simple toy examples to calrify these concepts.\n",
    "\n",
    "### 1.B.a. Softmax in NumPy (1 point)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Let's start by implementing the mentioned softmax function. In the cell below, **for 1 point** implement the softmax function from scratch using only NumPy operators.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> Do not use for loops!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf5f7b7e03c400b7d2be932d9a8ae170",
     "grade": false,
     "grade_id": "cell-0a60482b7d7439e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the softmax of an input vector x\n",
    "def softmax(x):\n",
    "    out = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c754759bdd1a8e8e8f4df41546e2b0ed",
     "grade": false,
     "grade_id": "cell-f5176d243f6c6814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As usual, run the next cell for a quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fbb825931f105c2fbdbb312594972c9",
     "grade": true,
     "grade_id": "cell-0460b5eb0e50162d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Input vector\n",
    "x = np.array([2.0, 1.0 ,0.1])\n",
    "# Calculate softmax\n",
    "outputs = softmax(x)\n",
    "print(f'Your softmax output for x={x} is {np.round(outputs, 4)}')\n",
    "# Check that the sum is 1\n",
    "assert np.round(sum(outputs), 4) == 1, f'The sum of the softmax output should always be 1, yours is {sum(outputs):.4f}'\n",
    "print('Nice, you passed the sanity check. This does guarantee the points though!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e268bb2b9f21dc708bf001aa9371f31b",
     "grade": false,
     "grade_id": "cell-c3a2efe14cde57f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.b. Softmax in PyTorch\n",
    "[Back to index](#Index)\n",
    "\n",
    "As you may already have guessed, we can also use PyTorch to calculate the softmax. For this we can make use of the function [`torch.nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html), which we will now use to test your NumPy implementation. Run the cell below to check that your `softmax` function produces the same result as the one from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c4b812a94bca7132371197a211a08e0",
     "grade": false,
     "grade_id": "cell-12526e65a87e2d1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the input tensor\n",
    "x_tc = torch.tensor([2.0, 1.0 ,0.1])\n",
    "# Calculate the softmax with PyTorch\n",
    "softmax_tc = torch.nn.Softmax(dim=0)\n",
    "# torch.nn.Softmax() returns a function which we can evaluate on any input\n",
    "outputs_tc = softmax_tc(x_tc)\n",
    "print(f'The PyTorch softmax output is {outputs_tc}')\n",
    "# Check that the outputs are equal\n",
    "np.testing.assert_array_almost_equal(outputs_tc, outputs, err_msg = 'Your NumPy implementation does not match the PyTorch output!')\n",
    "print('Great! Your sofmax produced the same result as PyTorch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13546782a4c7e33f81a5dbde5ad4a94e",
     "grade": false,
     "grade_id": "cell-ce1db3a666b11e65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.c. Cross-entropy in NumPy (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Cross-entropy is commonly used in classification tasks both in traditional machine learning and deep learning. It is defined as\n",
    "\n",
    "$$\\operatorname{H}(y, t) = -\\sum_{i=1}^Ct_i\\log(y_i)$$\n",
    "with $y$ the prediction and $t$ the target vector.\n",
    "\n",
    "and it measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy increases as the predicted probability diverges from the actual label. So predicting a probability of $y = [0.01, 0.00, 0.99]$ when the actual observation label is $t = [1,0,0]$ would be bad and result in a high loss value. A perfect prediction on the other hand would result in a cross-entropy loss of 0.\n",
    "\n",
    "In the next cell, **for 1 point**, implement the `cross_entropy` function that calculates the cross entropy loss as defined in the formula above from scratch, only using NumPy operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af0be3e44242816299576b084d6b307b",
     "grade": false,
     "grade_id": "cell-5baf9049b77dc840",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function that calculates the cross-entropy loss\n",
    "def cross_entropy(predicted, target):\n",
    "    loss = None\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74de9e9be6fc438c8d93a8d40de2d299",
     "grade": false,
     "grade_id": "cell-13f79525d64b37e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell for a simple sanity check that checks your function for a very good and a very bad example prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f24e6a42e7e56fc694fd7ec9860565",
     "grade": true,
     "grade_id": "cell-e1442867cbccbfe3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check that the loss of a very good prediction is 0\n",
    "good_prediction = np.round(cross_entropy(np.array([1, 1e-5, 1e-5]), np.array([1, 0, 0])), 4)\n",
    "assert good_prediction == 0, f'The cross-entropy loss of a very good prediction should be 0. Your loss is {good_prediction}.'\n",
    "# Check that the loss of a very bad prediction is large\n",
    "bad_prediction = np.round(cross_entropy(np.array([1e-5, 1, 1]), np.array([1, 0, 0])), 4)\n",
    "assert bad_prediction == 11.5129, f'The cross-entropy loss of a very bad prediction should be large (11.5129). Your loss is {bad_prediction}.'\n",
    "print('Good, your cross_entropy function provides the correct result for these simple test cases. Make sure to double check it anyway!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e266e637c3b8ad07d4e0455cb539bc",
     "grade": false,
     "grade_id": "cell-6cfc3c57f88a127e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, you can use the next cell to try different prediction-target combinations. This might help you in the upcoming MCQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e239feecd87fa0fc9cf8444da315a3",
     "grade": false,
     "grade_id": "cell-7e25abf439e9eda2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember that target is one-hot encoded (binary indicator of each class)\n",
    "# And the predictions vector should sum to 1\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354af9a7e6bdf3825a57dcadf62c20b5",
     "grade": false,
     "grade_id": "cell-9fdf6a7d1c8f4cab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multiple Choice Question\n",
    "\n",
    "Now at this point you may be asking yourself, does this make sense? If you looked closely at the cross-entropy values, there must be some intuitive order among the errors given the predictions. That said, for **1 point** answer the following. \n",
    "\n",
    "* Q1. Assume we calculate the cross-entropy errors `error_1 = cross_entropy([0.7, 0.2, 0.1], [1, 0, 0])`, `error_2 = cross_entropy([0.1, 0.3, 0.6], [1, 0, 0])`, and `error_3 = cross_entropy(softmax([2.0, 1.0 ,0.1]), [1, 0, 0])`.<br>What will be the order of the errors, from low to high? \n",
    "\n",
    "1. `error_1` < `error_2` < `error_3`\n",
    "2. `error_1` < `error_3` < `error_2`\n",
    "3. `error_2` < `error_1` < `error_3`\n",
    "4. `error_2` < `error_3` < `error_1`\n",
    "5. `error_3` < `error_1` < `error_2`\n",
    "6. `error_3` < `error_2` < `error_1`\n",
    "\n",
    "Modify the variable answer in the following cell to reflect your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "293c79c17aa9fbcd103b2cf80a13e22b",
     "grade": false,
     "grade_id": "cell-ff1237d6e7172c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Assign your answer to this variable\n",
    "answer = None\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6807f3e7490b86738fa281dd3d152741",
     "grade": true,
     "grade_id": "cell-aeb5fc657f4a288f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert answer in [1, 2, 3, 4, 5, 6], 'Choose one of 1, 2, 3, 4, 5 or 6.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "597d056d8b301229e69d2dfa7ba829a6",
     "grade": false,
     "grade_id": "cell-8854972520890683",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.B.d. Cross-entropy in PyTorch\n",
    "[Back to index](#Index)\n",
    "\n",
    "Of course PyTorch also provides a cross-entropy loss function: [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). However, this function combines the softmax together with the original cross-entropy function, which are averaged across observations for each minibatch of size $N$, so that the input to the PyTorch cross-entropy function is not the output of the softmax, but simply the `logits`, which is the raw unbounded net output:\n",
    "\n",
    "$$\\text{CrossEntropyLoss}()= \\frac{\\sum^{N}_{n=1} \\mathrm{H}(x^{(n)}, i^{(n)})}{N},$$\n",
    "$$\\text{where}\\quad\\mathrm{H}(x, i) = -\\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}\\right)$$\n",
    "with $N$ the number of samples per minibatch, $x$ the prediction and $i$ the target class index.\n",
    "\n",
    "In addition, the target input of the [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) is the **target class index** instead of the one-hot encoded vector used before.\n",
    "\n",
    "Let's check now if the results obtained from PyTorch match the ones from our own NumPy function. Run the cell below to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "259628b64c738cbdcc238e4128bfa094",
     "grade": false,
     "grade_id": "cell-4577942a3f8d66d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NumPy input and target vector\n",
    "x = np.array([2.0, 1.0 ,0.1])\n",
    "target = np.array([1, 0, 0])\n",
    "# Calculate softmax\n",
    "x_sm = softmax(x)\n",
    "# Calculate NumPy cross-entropy\n",
    "outputs = cross_entropy(x_sm, target)\n",
    "\n",
    "# Create the input and target tensor (needs to be 2-dimensional)\n",
    "x_tc = torch.tensor([x])\n",
    "# The class index 0 corresponds to [1, 0, 0]\n",
    "target_tc = torch.tensor([0])\n",
    "# Get the PyTorch cross entropy function and evaluate it\n",
    "cross_entropy_loss_tc = torch.nn.CrossEntropyLoss()\n",
    "outputs_tc = cross_entropy_loss_tc(x_tc, target_tc)\n",
    "\n",
    "print(f'Your cross-entropy output is {outputs:.4f}.')\n",
    "print(f'The PyTorch cross-entropy output is {outputs_tc:.4f}.')\n",
    "# Check that the outputs are equal\n",
    "np.testing.assert_array_almost_equal(outputs_tc, outputs)\n",
    "print('Nice! Your cross-entropy produced the same result as PyTorch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bff497d6e59df239b6e2692d416a5660",
     "grade": false,
     "grade_id": "cell-24f59945d8af5157",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.C. Build a model and train it with PyTorch\n",
    "### 1.C.a. Define the model (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "To begin this section, and **for 2 points, complete the function `build_model`** that defines a model with three linear layers ([`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) (input, hidden and output) of which the first two also have a ReLU activation function ([`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)). The output layer does not have an activation function since we want the output to be the `logits`, which we can then use as an input to the cross-entropy loss function. Use the [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module to build the model and run the cell below the next one to check that your answers are valid. Note that `build_model` has no input parameters, and the described model as output.\n",
    "\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "**Note:** If you don't remember how to build a model using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html), look at section 1.B.c. of the [previous notebook](./1_NN_Basics.ipynb) again.  \n",
    "</div> \n",
    "<div class=\" alert alert-warning\">\n",
    "    \n",
    "**Important:** We want the hidden layer to have 64 neurons. You need to choose the appropriate input and output size corresponding to our data! You will need to hardcode it in the function, as it has no input parameters.\n",
    "</div> \n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "**Hint:** Our goal is to classify an RGB pixel into one of three classes. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71f5fc4487ebdd1b5c8e43cdea800f00",
     "grade": false,
     "grade_id": "cell-46deb6e98adaac42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build a model with the given hyper-parameters\n",
    "model = None\n",
    "\n",
    "def build_model():\n",
    "    # Hard-code hyper-parameters (change values)\n",
    "    input_size = None\n",
    "    hidden_size = 64\n",
    "    output_size = None\n",
    "    # Initialize model variable (to redefine)\n",
    "    model = None\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return model\n",
    "\n",
    "# Declare the model\n",
    "model = build_model()\n",
    "print(f'The total number of parameters in your model is: {sum([np.prod(list(pnb.size())) for pnb in model.parameters()])}\\n')\n",
    "print(f'This is your model:\\n{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f63027d6dc1bac53ded4a6094912d9c3",
     "grade": false,
     "grade_id": "cell-ac0b6188d5857867",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell to perform an elaborate test on your model. It checks everything from the number and type of layers up to the input and output sizes of the layers, so that you can be sure that you have a correct model to continue the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125e34c9827e2ed4c6bbe0582093d183",
     "grade": true,
     "grade_id": "cell-40ac96f5efdd3c35",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First we redeclare the model, to be sure\n",
    "model = build_model()\n",
    "# Check that the model has 5 layers\n",
    "assert len(model._modules.items()) == 5, f'The model should have a total of 5 layers: 3 linear and 2 ReLU. Yours currently has {len(model._modules.items())} layers.'\n",
    "# Check that the layer types are correct\n",
    "for i, layer in enumerate(model._modules.items()):\n",
    "    current_layer = torch.nn.modules.linear.Linear if i % 2 == 0 else torch.nn.modules.activation.ReLU\n",
    "    assert type(layer[1]) == current_layer, f'Layer {i} should be of type {current_layer}, not {type(layer[1])}.'\n",
    "# Check that the input and output size of the layers is correct and that torch.nn.Sequential was used\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if i == 0:\n",
    "        assert type(module) == torch.nn.modules.container.Sequential, 'You should use torch.nn.Sequential to build your model!'\n",
    "    if i == 1:\n",
    "        assert module.in_features == 3, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that we are trying to classify RGB pixels.'\n",
    "        assert module.out_features == 64, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "    if i == 3:\n",
    "        assert module.in_features == 64, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "        assert module.out_features == 64, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "    if i == 5:\n",
    "        assert module.in_features == 64, f'The input size of layer {i-1} ({module.in_features}) is not correct. Remember that the hidden layer should be of size 64.'\n",
    "        assert module.out_features == 3, f'The output size of layer {i-1} ({module.out_features}) is not correct. Remember that the output size should be equal to the number of classes we have.'\n",
    "print('Well done! The model seems to be correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee95cd572256aa2e97f0e4d89c912511",
     "grade": false,
     "grade_id": "cell-bf59ac1f0906bb29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.b. Complete the training pipeline (2 points)\n",
    "[Back to index](#Index)\n",
    "\n",
    "Now that we have the desired model, we need to implement the training workflow that optimizes our model. The basic structure has already been created so you only need to implement some of the important steps. In the next cell, **for 2 points**, complete the function `train` that takes as input parameters:\n",
    "* `model` : A PyTorch model, e.g. the output of `build_model`,\n",
    "* `train_loader` : The training `DataLoader` that we defined in section [1.A.c.](#1.A.c.-Creating-the-DataLoader),\n",
    "* `valid_loader` : The validation `DataLoader` that we defined in section [1.A.c.](#1.A.c.-Creating-the-DataLoader),\n",
    "* `optimizer` : An optimization function from the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) module. Here we use [`torch.optim.Adam`](https://pytorch.org/docs/master/generated/torch.optim.Adam.html),\n",
    "* `loss_fn` : A loss function of the form `loss_fn(predicted, target)`. Here we use [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html),\n",
    "* `max_iter` : The maximum number of training batches to use (defaults to 50),\n",
    "* `save_model` : Tells the function if it should save the best model during training. Defaults to `True`, and \n",
    "* `save_model_name` : The name under which the model will be saved, defaults to `'best_model'`.\n",
    "\n",
    "and returns the evolution (history) of the training and validation loss as well as the highest validation accuracy achieved.\n",
    "\n",
    "<div class = 'alert alert-info'>\n",
    "    <b>Hint:</b> If in doubt, you can take inspiration from the training workflows we defined in the first part of the notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac11a991bb8643aaee925c05008175c",
     "grade": false,
     "grade_id": "cell-533966e8c5d0eed6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model', save_model=True):\n",
    "    # Some initializations\n",
    "    lowest_loss = np.inf\n",
    "    highest_accuracy = 0.\n",
    "    train_loss, valid_loss, train_accuracy, valid_accuracy = [], [], [], []\n",
    "    train_batch_loss, correct, lowest_iteration = 0, 0, 0\n",
    "    \n",
    "    # Tell the model that we are in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate through the training minibatches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Extract the train and target minibatches\n",
    "        x_i, y_i = data[0], data[1]\n",
    "        \n",
    "        # Get the current loss value and training predictions (forward pass)\n",
    "        # Use the same variable names as the ones initialized next\n",
    "        loss, y_hat_i = None, None        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Perform the backward pass (remember to first reset the gradients)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Perform one training step (optimization)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Calculate the average loss\n",
    "        train_batch_loss = float(loss) / len(y_i)\n",
    "        train_loss.append(train_batch_loss)\n",
    "\n",
    "        # Calculate the validation loss and accuracy\n",
    "        with torch.no_grad():\n",
    "            # Tell the model that we are in evaluation mode\n",
    "            model.eval()\n",
    "            valid_batch_loss, correct = 0, 0\n",
    "            # Iterate through the validation minibatches\n",
    "            for x_i, y_i in valid_loader:\n",
    "                \n",
    "                # Get the current validation loss value\n",
    "                loss = None\n",
    "                # YOUR CODE HERE\n",
    "\n",
    "                # Accumulate the loss\n",
    "                valid_batch_loss += float(loss)\n",
    "                # Get the predicted class (max probability)\n",
    "                _, predicted = torch.max(y_hat_i.data, 1)\n",
    "                # Accumulate accuracy\n",
    "                correct += (predicted == y_i).sum().item()\n",
    "\n",
    "            # Calculate average loss and accuracy\n",
    "            valid_batch_loss = valid_batch_loss / len(valid_loader)\n",
    "            valid_batch_accuracy = 100 * correct / (len(valid_loader) * batch_size)\n",
    "            valid_accuracy.append(valid_batch_accuracy)\n",
    "            valid_loss.append(valid_batch_loss)\n",
    "            \n",
    "        # Save the model if we had an improvement\n",
    "        if valid_batch_loss <= lowest_loss:\n",
    "            lowest_loss = valid_batch_loss\n",
    "            lowest_iteration = i        \n",
    "            highest_accuracy = valid_batch_accuracy\n",
    "            if save_model:\n",
    "                torch.save(model.state_dict(), save_model_name + '.pt')\n",
    "        \n",
    "        # Print current model state\n",
    "        print(f'Iteration {i+1:2}: train loss={train_batch_loss:8.4f}  valid_loss={valid_batch_loss:8.4f}  valid_acc={valid_batch_accuracy:5.2f} %  best accuracy(@iter{lowest_iteration:2})={highest_accuracy:5.2f}')\n",
    "        # Stop after 50 iterations\n",
    "        if (i+1) % max_iter == 0:\n",
    "            break \n",
    "        \n",
    "            \n",
    "    return train_loss, valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217878b3d34ed382d4c1dd66f98b8f84",
     "grade": false,
     "grade_id": "cell-afb2d5657dd7bc24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's try to train your model! Run the next cell to perform a training using only 50 of the 180 training minibatches. If you implemented everything correctly, you should see how both the training and validation loss decrease and the accuracy increases as the training advances.\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "<b>Warning:</b> We limit the number of minibatches here so that you don't have to wait too long for\n",
    "    the training to finish since Noto does not provide a lot of computing power. Still, the training might take a few minutes to run... \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde35515b4ce33033f26fcb1c54ee3b6",
     "grade": true,
     "grade_id": "cell-a06dc949523babcc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Redefine model\n",
    "model = build_model()\n",
    "\n",
    "# Training related hyper-parameters\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# set the loss function loss_fn\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# set the Adam optimizer with the given learning rate \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Get DataLoaders\n",
    "train_loader = DataLoader(dataset=CellDataset(net_train_input, net_train_target), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=CellDataset(net_valid_input, net_valid_target), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss, valid_loss, valid_acc = train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model_without_weights')\n",
    "print('\\nFinished training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8c2d8fecb581ce1927b4b1335f38235",
     "grade": false,
     "grade_id": "cell-346dc7456c04020a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to see how the training and validation loss evolved during training. If you implemented everything correctly, you should be able to see how the validation loss starts off relatively high, but converges to 0 as the training iterations increase. The training loss already starts very small and stays small during the entire training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d739c4bab4cd40a9ad93bea2b4c245c8",
     "grade": true,
     "grade_id": "cell-de983118a48c0147",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize = (9, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax_loss = plt.gca()\n",
    "ax_loss.set_xlabel('Iterations')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "plt.title('Training and Validation Loss and Accuracy History')\n",
    "p1 = ax_loss.plot(train_loss, 'y', label = 'Train Loss')\n",
    "p2 = ax_loss.plot(valid_loss, 'r', label = 'Val Loss')\n",
    "\n",
    "# Get twin axis and plot accuracy\n",
    "ax_acc = ax_loss.twinx()  \n",
    "ax_acc.set_ylabel('Accuracy') \n",
    "p3 = ax_acc.plot(valid_acc, 'k', label = 'Val Accuracy')\n",
    "legends = [l.get_label() for l in p1+p2+p3]\n",
    "plt.grid()\n",
    "ax_acc.legend(p1+p2+p3, legends)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9da1fc9d4e0bf7e49ea22a48bc6ee97",
     "grade": false,
     "grade_id": "cell-ecfb20a7a1a94caa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.c Test the model on a test image\n",
    "[Back to index](#Index)\n",
    "\n",
    "Now that the training is complete, we can test our model on a test image. Run the next cell to view the test image and the target image that we are trying to obtain.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> If you click on <code>Options</code>, you can enable the <code>Joint Zoom</code> and zoom into different areas of the image to see how the pixels are classified.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fe4940b36ff7fce3c37238beb0fca0e",
     "grade": false,
     "grade_id": "cell-00124ba2ae7f3090",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "view = viewer([input_img_test, label_img_test], titles=['Test Image', 'Ground Truth'], subplots=(1,2), widgets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f69e8293ce7381f48245358945d10339",
     "grade": false,
     "grade_id": "cell-5be9b342520d0204",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, we will load the best-performing model from the disk using [`torch.load`](https://pytorch.org/docs/stable/generated/torch.load.html) together with [`load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict), since we saved it during training. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74fbd117910ff57bbd8dca6fb990fdcd",
     "grade": false,
     "grade_id": "cell-b0ada0529f0b09a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./best_model_without_weights.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a71703c742e04bb93e5cbfc31a51830e",
     "grade": false,
     "grade_id": "cell-a1e3563a61adcf19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we convert the input test image to a PyTorch tensor and extract the correct classes from the target test image as we did for the training and test validation sets. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02a10cb0e33cc94f92095bb6856c11f3",
     "grade": false,
     "grade_id": "cell-48a4133d70ebad4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create test labels (with the correct RGB calues and the indexes that should have these values)\n",
    "pixel_rgb_val_test, test_label_class_index = np.unique(label_img_test.reshape((-1, 3)), axis=0, return_inverse=True)\n",
    "# Flatten the input and make it of type float\n",
    "net_input_test = torch.FloatTensor(input_img_test.reshape((-1, 3)))\n",
    "# Make the target class a tensor of type long\n",
    "net_target_test = torch.LongTensor(test_label_class_index)\n",
    "print(f'The three unique RGB values of the labeled image are:\\n{pixel_rgb_val_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33489bf4eee7398a7fe12ac8ffe28152",
     "grade": false,
     "grade_id": "cell-1836a642f4da92b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can simply provide the test image as an input to our model and compare it's output with the test target. Run the next cell to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c219b9a80035a27c5ae30fd27f990990",
     "grade": false,
     "grade_id": "cell-1efa253352982d1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Tell the model that we are in evaluation mode\n",
    "model.eval()\n",
    "# Generate prediciton\n",
    "y_hat = model(net_input_test)\n",
    "# Extract the corresponding classes from the one-hot encoded output vecotor\n",
    "_, predicted = torch.max(y_hat.data, 1)\n",
    "# Calculate the accuracy\n",
    "correct = (predicted == net_target_test).sum().item()\n",
    "test_accuracy = correct / len(net_target_test) * 100\n",
    "print(f'Test accuracy: {test_accuracy:.2f} %\\n')\n",
    "# Create the model output image\n",
    "predicted_img = pixel_rgb_val_test[predicted.data.cpu().numpy()].reshape(input_img_test.shape)\n",
    "# Display\n",
    "plt.close('all')\n",
    "view = viewer([input_img_test, predicted_img, label_img_test], title=['Test image', 'Model output', 'Correct classification'], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51f0b9f0ca36c1428710afd8195487e8",
     "grade": false,
     "grade_id": "cell-b219cc26487182ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** It was actually expected that one of the three classes is very badly classified! Check the next section to find out why and how to compensate for that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acb4a031df734bc1ea595034c24e4bee",
     "grade": false,
     "grade_id": "cell-37978d2dcf426d79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.C.d. Classwise accuracy\n",
    "[Back to index](#Index)\n",
    "\n",
    "As you saw above, the test accuracy for the entire dataset is fairly okay ($\\sim 80\\%$). However, the output results do not seem to classify correctly one of the classes. Why does this happen? To investigate this, let's first look into the classwise accuracy, instead of the total accuarcy, meaning that we calculate the accuracy of each class separately. Run the cell below to see how the model performs on each one of the three classes.\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note:</b> The accuracy changes each time you re-train your model because the dataset is shuffled again each time. If you want to see a good improvement in the next section, re-run the training until you have a very low accuracy for class 0. Normally this is not at all what we want from a model, but here it is preferable for illustration purposes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbe1b2c79f271c1d48b6f29aca8b217c",
     "grade": false,
     "grade_id": "cell-8f23232a7cf875ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cls0_acc_wo = 100 * np.logical_and((predicted == 0).data.cpu(), (net_target_test == 0).data.cpu()).sum().item()/(net_target_test == 0).sum().item()\n",
    "cls1_acc_wo = 100 * np.logical_and((predicted == 1).data.cpu(), (net_target_test == 1).data.cpu()).sum().item()/(net_target_test == 1).sum().item()\n",
    "cls2_acc_wo = 100 * np.logical_and((predicted == 2).data.cpu(), (net_target_test == 2).data.cpu()).sum().item()/(net_target_test == 2).sum().item()\n",
    "print('Test accuracy (classwise)\\nclass 0: %.2f %% class 1: %.2f %% class 2: %.2f %%' % (cls0_acc_wo,cls1_acc_wo,cls2_acc_wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fb463b635395d62c11063b0b2b9720c",
     "grade": false,
     "grade_id": "cell-31ae974972879de7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you see, the model performs much worse at identifying class $0$ compared to the other two classes. This phenomenon is called the class imbalance problem and is a result of the fact that some of our classes, like class $0$, are underrepresented in the training data. You can easily see this if you look at the [`label_img`](#1.-Multiclass-pixel-classification) image and think about how many of the total pixels belong to which class.\n",
    "\n",
    "## 1.D. Class imbalance problem\n",
    "[Back to index](#Index)\n",
    "\n",
    "First of all, let's check how well our three classes are actually represented in our dataset. Run the cell below to calculate the ratio each one of the classes compared to the total amount of pixels in our training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a6f7d5ec5d21160a179a58dd49b9c29",
     "grade": false,
     "grade_id": "cell-8fe973d5f5f41802",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_, train_label_class_counts = np.unique(label_img.reshape((-1, 3)), axis=0, return_counts=True)\n",
    "# Compute class ratios\n",
    "class_ratio = train_label_class_counts/sum(train_label_class_counts)\n",
    "print(f'Ratio of the number of pixels for each class:\\nClass 0: {class_ratio[0]*100:.2f}%, Class 1: {class_ratio[1]*100:.2f}%, Class 2: {class_ratio[2]*100:.2f}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc915f4fa9a07f5be933e0a3746ee7d",
     "grade": false,
     "grade_id": "cell-3c33bb42114373a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, class $0$ only represents $2\\%$ of all pixels in the training image, which makes it clear why our network had a hard time classifying it correctly.\n",
    "\n",
    "Unfortunately, this is a common issue in practice, especially in the medical imaging area, where abnormal data are rare compared to normal cases. The question remains, how can compensate for that? \n",
    "\n",
    "### 1.D.a. Add weights to the loss function (1 point)\n",
    "[Back to index](#Index)\n",
    "\n",
    "One of the easiest ways to address this issue is to add a weight $w_i$ to the loss function that corresponds to the representation of class $i$. Our new cross-entropy loss thus becomes: \n",
    "$$\\operatorname{H}(x, i) = w_i\\left(-\\log\\left(\\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}\\right)\\right)$$\n",
    "\n",
    "The final batch loss is then normalized for each minibatch of size $N$:\n",
    "\n",
    "$$\\operatorname{BatchLoss} =  \\frac{\\sum^{N}_{n=1}\\operatorname{H}(x^{(n)}, i^{(n)})}{\\sum^{N}_{n=1} w_{i^{(n)}}}.$$\n",
    "\n",
    "There are many different ways to calculate these weights, but here we will simply use <b>the inverse of the class ratios stored in the parameter `class_ratio`</b> that we calculated in the above cell. To add the weights to the cross-entropy loss function in PyTorch, you can simply use `loss_fn = torch.nn.CrossEntropyLoss(weight=weights)`, but this step is already implemented for you. \n",
    "\n",
    "All you need to do in the next cell, **for 1 point**, is to complete the function `calc_weights` that calculates the correct weights according to the parameter `class_counts`, an array of the size $(n, )$ (where $n$ is the number of classes) and returns the weights in the parameter `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ccdb55c961ac16fce459b326cb9768e",
     "grade": false,
     "grade_id": "cell-4ff8aaa82c7322e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_weights(class_counts):\n",
    "    # Calculate the correct weights\n",
    "    weights = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Convert the weights to a tensor and re-create the loss function\n",
    "weights_tc = torch.FloatTensor(calc_weights(train_label_class_counts))\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ab0b3f253fb4e18994bfb5797e18054",
     "grade": false,
     "grade_id": "cell-0431f19b98456394",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's re-run the training with the new loss function and see if the classwise accuracy improved. Run the cell below to train your model again for 50 minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abf9cc9bbef6f584452c252941c8d738",
     "grade": true,
     "grade_id": "cell-93e5f12e10b8cbca",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Reinitialize the model for fair comparison, and link to new optimizer object  \n",
    "model = build_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "train_loss, valid_loss, valid_acc = train(model, train_loader, valid_loader, optimizer, loss_fn, max_iter=50, save_model_name='best_model_with_weights')\n",
    "print('\\nFinished training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7080533471a47c07e20d30a5741c05bb",
     "grade": false,
     "grade_id": "cell-45b80faec0a20de7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Did the overall accuracy improve? Run the cell below to visualize the training and validation loss history and the evolution of the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5369cad8b704ad138c3e2d17938310a1",
     "grade": false,
     "grade_id": "cell-ae6e6bc753a33b06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax_loss = plt.gca()\n",
    "ax_loss.set_xlabel('Iterations')\n",
    "ax_loss.set_ylabel('Loss')\n",
    "plt.title('Training and Validation Loss and Accuracy History')\n",
    "p1 = ax_loss.plot(train_loss, 'y', label = 'Train Loss')\n",
    "p2 = ax_loss.plot(valid_loss, 'r', label = 'Val Loss')\n",
    "\n",
    "# Get twin axis and plot accuracy\n",
    "ax_acc = ax_loss.twinx()  \n",
    "ax_acc.set_ylabel('Accuracy') \n",
    "p3 = ax_acc.plot(valid_acc, 'k', label = 'Val Accuracy')\n",
    "legends = [l.get_label() for l in p1+p2+p3]\n",
    "plt.grid()\n",
    "ax_acc.legend(p1+p2+p3, legends)\n",
    "plt.show()\n",
    "\n",
    "# Load best model and set to evaluation mode\n",
    "model.load_state_dict(torch.load('best_model_with_weights.pt'))\n",
    "model.eval()\n",
    "# Calculate test accuracy\n",
    "y_hat = model(net_input_test)\n",
    "_, predicted = torch.max(y_hat.data, 1)\n",
    "correct = (predicted == net_target_test).sum().item()\n",
    "test_accuracy = correct / len(net_target_test) * 100\n",
    "print('Test accuracy: %.2f %%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "624d94a73acb16d73378d1cd77a17353",
     "grade": false,
     "grade_id": "cell-566db75dec1791e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now the total test accuracy might not have changed much, but if we calculate the classwise accuracy again, you should see a major improvement for class $0$. \n",
    "\n",
    "Run the cell below to do so and ensure that class $0$ is now classified much better than before. Moreover, we will the input image, the ground truth and the model output. The improvement should be evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e24248d3ed481dee2887d55852e0df95",
     "grade": false,
     "grade_id": "cell-b1301b8ae1553d96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate classwise accuracy\n",
    "cls0_acc_w =  100 * np.logical_and((predicted == 0).data.cpu(), (net_target_test ==0).data.cpu()).sum().item()/(net_target_test ==0).sum().item()\n",
    "cls1_acc_w = 100 * np.logical_and((predicted == 1).data.cpu(), (net_target_test ==1).data.cpu()).sum().item()/(net_target_test ==1).sum().item()\n",
    "cls2_acc_w = 100 * np.logical_and((predicted == 2).data.cpu(), (net_target_test ==2).data.cpu()).sum().item()/(net_target_test ==2).sum().item()\n",
    "print('Test accuracy (classwise)\\nclass 0: %.2f %% class 1: %.2f %% class 2: %.2f %%\\n' % (cls0_acc_w,cls1_acc_w,cls2_acc_w))\n",
    "# Create predicted image\n",
    "predicted_img = pixel_rgb_val[predicted.data.cpu().numpy()].reshape(input_img_test.shape)\n",
    "# Display\n",
    "plt.close('all')\n",
    "view = viewer([input_img_test, predicted_img, label_img_test], title=['Test image', 'Model output', 'Correct classification'], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1cc744305795ce714db00ff03db777e",
     "grade": false,
     "grade_id": "cell-b80d64c4f0cca561",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And finally, let's print both classwise accuracies together for an easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35c7d1d61d65d1070fa9c90cc22f5749",
     "grade": false,
     "grade_id": "cell-088817084e8437ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'(without weights) class 0: {cls0_acc_wo:.2f} %, class 1: {cls1_acc_wo:.2f} %, class 2: {cls2_acc_wo:.2f} %')\n",
    "print(f'(with weights)  class 0: {cls0_acc_w:.2f} %, class 1: {cls1_acc_w:.2f} %, class 2: {cls2_acc_w:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3f29ef9144fef0aa2a6e037712f3c52",
     "grade": false,
     "grade_id": "cell-aadce1c4e8e2d419",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<p><b>Congratulations on finishing the second part of the Neural Networks lab!</b></p>\n",
    "<p>\n",
    "Make sure to save your notebook (you might want to keep a copy on your personal computer) and upload it to <a href=\"https://moodle.epfl.ch/mod/assign/view.php?id=1157357\">Moodle</a>, in a zip file with other notebooks of this lab.\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "* Keep the name of the notebook as: *2_NN_Application.ipynb*,\n",
    "* Name the zip file: *Neural_Networks_Lab.zip*.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<h4>Feedback</h4>\n",
    "    <p style=\"margin:4px;\">\n",
    "    This is the first edition of the image-processing laboratories using Jupyter Notebooks running on Noto. Do not leave before giving us your <a href=\"https://moodle.epfl.ch/mod/feedback/view.php?id=1157363\">feedback here!</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
